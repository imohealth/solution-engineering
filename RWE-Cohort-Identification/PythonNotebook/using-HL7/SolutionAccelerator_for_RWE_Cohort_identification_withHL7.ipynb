{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "833934ef",
   "metadata": {},
   "source": [
    "<img src=\"../static/imo_health.png\" alt=\"IMO Health Logo\" width=\"300\"/>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3189cadb",
   "metadata": {},
   "source": [
    "# Cohort Identification in RWE Solution Accelerator\n",
    "\n",
    "This notebook provides an end-to-end solution for identifying patient cohorts from Real World DATA (RWD)  and matching them against cohort eligibility criteria. The workflow includes:\n",
    "\n",
    "1. **HL7 Data Extraction**: Parse HL7 messages to extract medical codes (ICD-10-CM, CPT, LOINC, SNOMED CT)\n",
    "2. **Code Normalization**: Use IMO's normalization API to enhance incomplete or missing codes\n",
    "3. **Cohort Dictionary Search**: Search and download Cohort criteria valuesets from IMO's FHIR API\n",
    "4. **Cohort Matching**: Match patient codes against cohort eligibility criteria to identify eligible cohorts\n",
    "5. **Results Analysis**: Generate match reports and statistics\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- IMO API credentials configured in `config.json`\n",
    "- HL7 data files in `../uploads/hl7_data/` directory\n",
    "- Required Python packages: `boto3`, `requests`, `hl7apy`, `pandas`\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1: Environment Setup and Package Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80566eeb",
   "metadata": {},
   "source": [
    "### Install Required Python Packages\n",
    "\n",
    "This cell installs the necessary Python packages for the solution:\n",
    "\n",
    "- **`boto3`**: AWS SDK for Python (if cloud integration is needed)\n",
    "- **`requests`**: HTTP library for API calls to IMO services\n",
    "- **`hl7apy`**: Python library for parsing and processing HL7 messages\n",
    "- **`pandas`**: Data manipulation and analysis library for handling datasets\n",
    "\n",
    "> **Note**: Uncomment the pip install command if running in a new environment or if packages are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "548faa13-c63b-448f-b2bb-b16ec14fc65f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in d:\\users\\skale\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.34.0)\n",
      "Requirement already satisfied: requests in d:\\users\\skale\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: hl7apy in d:\\users\\skale\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.3.5)\n",
      "Requirement already satisfied: pandas in d:\\users\\skale\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: botocore<1.35.0,>=1.34.0 in d:\\users\\skale\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from boto3) (1.34.162)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in d:\\users\\skale\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.10.0,>=0.9.0 in d:\\users\\skale\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from boto3) (0.9.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in d:\\users\\skale\\appdata\\roaming\\python\\python313\\site-packages (from botocore<1.35.0,>=1.34.0->boto3) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in d:\\users\\skale\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from botocore<1.35.0,>=1.34.0->boto3) (2.6.2)\n",
      "Requirement already satisfied: six>=1.5 in d:\\users\\skale\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.0->boto3) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\users\\skale\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\users\\skale\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (3.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\users\\skale\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (2026.1.4)\n",
      "Requirement already satisfied: numpy>=1.26.0 in d:\\users\\skale\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.4.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\users\\skale\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\users\\skale\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install required packages (uncomment if running in a new environment)\n",
    "!pip install boto3 requests hl7apy pandas "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493494cd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: Import Core Libraries\n",
    "\n",
    "### Import Essential Libraries for Data Processing\n",
    "\n",
    "This cell imports the fundamental libraries needed for the solution:\n",
    "\n",
    "- **`boto3`**: Amazon Web Services SDK (for potential cloud storage integration)\n",
    "- **`hl7apy`**: Specialized library for parsing HL7 healthcare data messages\n",
    "- **`requests`**: HTTP client for making API calls to IMO's FHIR and normalization services\n",
    "- **`json`**: Built-in library for handling JSON data from API responses\n",
    "\n",
    "These imports establish the foundation for HL7 message parsing, API communication, and data processing throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65c8a22d-33c6-4ec0-ab77-f72324616707",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import hl7apy\n",
    "from hl7apy.parser import parse_message\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ab55e0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: HL7 Data Loading and Processing\n",
    "\n",
    "### Load HL7 Messages from Local Upload Directory\n",
    "\n",
    "This cell performs the following operations:\n",
    "\n",
    "1. **Directory Setup**: Defines the path to the HL7 data upload folder (`uploads/hl7_data`)\n",
    "2. **File Discovery**: Recursively searches for all HL7 files in the upload directory\n",
    "3. **Data Loading**: Reads each HL7 file and stores the content in memory for processing\n",
    "4. **Preview Display**: Shows the first 200 characters of each file for verification\n",
    "\n",
    "**Expected Input**: HL7 message files (.hl7, .txt, or similar formats) containing patient data\n",
    "**Output**: `hl7_data_dict` list containing the raw HL7 message content from all files\n",
    "\n",
    "> **Important**: Ensure your HL7 data files are placed in the `uploads/hl7_data/` directory before running this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cdd0ec6-c97a-4f84-96ef-9d8b18d244a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../uploads_hl7/hl7_data\\\\hl7_data1', '../uploads_hl7/hl7_data\\\\hl7_data10', '../uploads_hl7/hl7_data\\\\hl7_data11', '../uploads_hl7/hl7_data\\\\hl7_data12', '../uploads_hl7/hl7_data\\\\hl7_data2', '../uploads_hl7/hl7_data\\\\hl7_data3', '../uploads_hl7/hl7_data\\\\hl7_data4', '../uploads_hl7/hl7_data\\\\hl7_data5', '../uploads_hl7/hl7_data\\\\hl7_data6', '../uploads_hl7/hl7_data\\\\hl7_data7', '../uploads_hl7/hl7_data\\\\hl7_data8', '../uploads_hl7/hl7_data\\\\hl7_data9']\n",
      "Loaded file: ../uploads_hl7/hl7_data\\hl7_data1\n",
      "MSH|^~\\&|LAB_APP|LAB_FACILITY|EMR_APP|EMR_FACILITY|202301151200||ORU^R01|MSG0001|P|2.5.1\n",
      "PID|1||PAT1^^^HOSPIT~PAT456^^^CLINIC||DOE1^JOHN||19900510|M|||123 MAIN ST^^ANYTOWN^CA^12345||555-123-4567|||M\n",
      "O...\n",
      "--------------------------------------------------\n",
      "Loaded file: ../uploads_hl7/hl7_data\\hl7_data10\n",
      "MSH|^~\\&|LAB_APP|LAB_FACILITY|EMR_APP|EMR_FACILITY|202301151202||ORU^R01|MSG0003|P|2.5.1\n",
      "PID|1||PAT003^^^HOSPIT~PAT003A^^^CLINIC||WILLIAMS^DAVID||19751130|M|||303 MAPLE AVE^^CHICAGO^IL^60601||312-555-...\n",
      "--------------------------------------------------\n",
      "Loaded file: ../uploads_hl7/hl7_data\\hl7_data11\n",
      "MSH|^~\\&|LAB_APP|LAB_FACILITY|EMR_APP|EMR_FACILITY|202301151203||ORU^R01|MSG0004|P|2.5.1\n",
      "PID|1||PAT004^^^HOSPIT~PAT004A^^^CLINIC||BROWN^JESSICA||19881111|F|||404 ELM ST^^MIAMI^FL^33101||305-555-3456||...\n",
      "--------------------------------------------------\n",
      "Loaded file: ../uploads_hl7/hl7_data\\hl7_data12\n",
      "MSH|^~\\&|LAB_APP|LAB_FACILITY|EMR_APP|EMR_FACILITY|202301151204||ORU^R01|MSG0005|P|2.5.1\n",
      "PID|1||PAT005^^^HOSPIT~PAT005A^^^CLINIC||DAVIS^MICHAEL||19930303|M|||505 BIRCH RD^^SEATTLE^WA^98101||206-555-67...\n",
      "--------------------------------------------------\n",
      "Loaded file: ../uploads_hl7/hl7_data\\hl7_data2\n",
      "MSH|^~\\&|LAB_APP|LAB_FACILITY|EMR_APP|EMR_FACILITY|202301151200||ORU^R01|MSG0001|P|2.5.1\n",
      "PID|1||PAT2^^^HOSPIT~PAT456^^^CLINIC||DOE2^JOHN||19900510|M|||123 MAIN ST^^ANYTOWN^CA^12345||555-123-4567|||M\n",
      "O...\n",
      "--------------------------------------------------\n",
      "Loaded file: ../uploads_hl7/hl7_data\\hl7_data3\n",
      "MSH|^~\\&|LAB_APP|LAB_FACILITY|EMR_APP|EMR_FACILITY|202301151200||ORU^R01|MSG0001|P|2.5.1\n",
      "PID|1||PAT3^^^HOSPIT~PAT456^^^CLINIC||DOE3^JOHN||19900510|M|||123 MAIN ST^^ANYTOWN^CA^12345||555-123-4567|||M\n",
      "O...\n",
      "--------------------------------------------------\n",
      "Loaded file: ../uploads_hl7/hl7_data\\hl7_data4\n",
      "MSH|^~\\&|LAB_APP|LAB_FACILITY|EMR_APP|EMR_FACILITY|202301151200||ORU^R01|MSG0001|P|2.5.1\n",
      "PID|1||PAT4^^^HOSPIT~PAT4^^^CLINIC||DOE4^JOHN||19900510|M|||123 MAIN ST^^ANYTOWN^CA^12345||555-123-4567|||M\n",
      "ORC...\n",
      "--------------------------------------------------\n",
      "Loaded file: ../uploads_hl7/hl7_data\\hl7_data5\n",
      "MSH|^~\\&|LAB_APP|LAB_FACILITY|EMR_APP|EMR_FACILITY|202301151200||ORU^R01|MSG0001|P|2.5.1\n",
      "PID|1||PAT5^^^HOSPIT~PAT5^^^CLINIC||DOE5^JOHN||19900510|M|||123 MAIN ST^^ANYTOWN^CA^12345||555-123-4567|||M\n",
      "ORC...\n",
      "--------------------------------------------------\n",
      "Loaded file: ../uploads_hl7/hl7_data\\hl7_data6\n",
      "MSH|^~\\&|IA PHIMS Stage^2.16.840.1.114222.4.3.3.5.1.2^ISO|IA Public Health Lab^2.16.840.1.114222.4.1.10411^ISO|IA.DOH.IDSS^2.16.840.1.114222.4.3.3.19^ISO|IADOH^2.16.840.1.114222.4.1.3650^ISO|201203142...\n",
      "--------------------------------------------------\n",
      "Loaded file: ../uploads_hl7/hl7_data\\hl7_data7\n",
      "MSH|^~\\&|LAB_APP|LAB_FACILITY|EMR_APP|EMR_FACILITY|202301151200||ORU^R01|MSG0001|P|2.5.1\n",
      "PID|1||PAT7^^^HOSPIT~PAT456^^^CLINIC||DOE7^JOHN||19900510|M|||123 MAIN ST^^ANYTOWN^CA^12345||555-123-4567|||M\n",
      "O...\n",
      "--------------------------------------------------\n",
      "Loaded file: ../uploads_hl7/hl7_data\\hl7_data8\n",
      "MSH|^~\\&|LAB_APP|LAB_FACILITY|EMR_APP|EMR_FACILITY|202301151200||ORU^R01|MSG0001|P|2.5.1\n",
      "PID|1||PAT001^^^HOSPIT~PAT001A^^^CLINIC||SMITH^JOHN||19800115|M|||101 OAK ST^^DALLAS^TX^75201||214-555-1234|||M...\n",
      "--------------------------------------------------\n",
      "Loaded file: ../uploads_hl7/hl7_data\\hl7_data9\n",
      "MSH|^~\\&|LAB_APP|LAB_FACILITY|EMR_APP|EMR_FACILITY|202301151201||ORU^R01|MSG0002|P|2.5.1\n",
      "PID|1||PAT002^^^HOSPIT~PAT002A^^^CLINIC||JOHNSON^EMILY||19920322|F|||202 PINE ST^^PHOENIX^AZ^85001||602-555-567...\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Load HL7 feed from uploads folder\n",
    "import os\n",
    "\n",
    "uploads_folder = '../uploads_hl7/hl7_data'\n",
    "\n",
    "def list_files_in_uploads_folder(folder_path):\n",
    "    \"\"\"\n",
    "    List all files in the uploads folder.\n",
    "    Returns a list of file paths.\n",
    "    \"\"\"\n",
    "    file_paths = []\n",
    "    if os.path.exists(folder_path):\n",
    "        for root, dirs, files in os.walk(folder_path):\n",
    "            for file in files:\n",
    "                file_paths.append(os.path.join(root, file))\n",
    "    return file_paths\n",
    "\n",
    "file_paths = list_files_in_uploads_folder(uploads_folder)\n",
    "print(file_paths)\n",
    "hl7_data_dict = []\n",
    "for file_path in file_paths:\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            hl7_data = file.read()\n",
    "            hl7_data_dict.append(hl7_data)\n",
    "            print(f\"Loaded file: {file_path}\")\n",
    "            print(hl7_data[:200] + \"...\" if len(hl7_data) > 200 else hl7_data)\n",
    "            print(\"-\" * 50)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e5d132",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: HL7 Message Parsing and Medical Code Extraction\n",
    "\n",
    "### Extract Medical Codes from HL7 Messages\n",
    "\n",
    "This comprehensive cell contains multiple extraction functions for different medical coding systems:\n",
    "\n",
    "#### Functions Included:\n",
    "\n",
    "1. **`extract_patient_id()`**: Extracts patient identifiers from PID segments\n",
    "2. **`extract_loinc_or_labtest_details()`**: Extracts LOINC codes from laboratory test results (OBX segments)\n",
    "3. **`extract_cpt_codes()`**: Extracts CPT procedure codes from PR1 and OBR segments\n",
    "4. **`extract_icd10cm_codes()`**: Extracts ICD-10-CM diagnosis codes from DG1 and OBX segments\n",
    "5. **`extract_snomedct_codes()`**: Extracts SNOMED CT codes from various segments\n",
    "\n",
    "#### Processing Workflow:\n",
    "\n",
    "- **Input**: Raw HL7 messages from `hl7_data_dict`\n",
    "- **Processing**: Each HL7 message is parsed and analyzed for medical codes\n",
    "- **Priority**: Processes codes in order: LOINC ‚Üí CPT ‚Üí ICD-10-CM ‚Üí SNOMED CT\n",
    "- **Output**: `trial_patient_dict` containing structured patient data with extracted codes\n",
    "\n",
    "#### Data Structure:\n",
    "Each extracted code includes:\n",
    "- `code`: The medical code value\n",
    "- `code_system`: The coding system (loinc, cpt, icd10cm, snomedct)\n",
    "- `description`: Human-readable description\n",
    "- `patient_id`: Associated patient identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "022275a4-bc06-4ff0-a626-24f49c6625ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'code': '17968-9', 'code_system': 'loinc', 'description': 'Bacteria6:Prid:Pt:Stool:Nom:Culture', 'patient_id': 'PAT1'}]\n",
      "[{'code': 'I10', 'code_system': 'icd10cm', 'description': 'Essential (primary) hypertension', 'patient_id': 'PAT003'}]\n",
      "[{'code': 'E78.5', 'code_system': 'icd10cm', 'description': 'Hyperlipidemia, unspecified', 'patient_id': 'PAT004'}]\n",
      "[{'code': 'F41.1', 'code_system': 'icd10cm', 'description': 'Generalized anxiety disorder', 'patient_id': 'PAT005'}]\n",
      "[{'code': 'I10', 'code_system': 'icd10cm', 'description': 'Essential (primary) hypertension', 'patient_id': 'PAT2'}]\n",
      "[{'code': '45380', 'code_system': 'cpt', 'description': 'Colonoscopy; with removal of polyp, snare technique', 'patient_id': 'PAT3'}]\n",
      "[{'code': '', 'code_system': 'cpt', 'description': 'Thoracic CT Scan with Contrast', 'patient_id': 'PAT4'}]\n",
      "[{'code': '80146002', 'code_system': 'snomedct', 'description': 'POST APPENDECTOMY FOLLOW-UP EXAM', 'patient_id': 'PAT5'}]\n",
      "[{'code': '625-4', 'code_system': 'loinc', 'description': 'Bacteria identified in Stool by Culture', 'patient_id': 'PAT6'}]\n",
      "[{'code': '', 'code_system': 'icd10cm', 'description': 'Systolic hypertension', 'patient_id': 'PAT7'}]\n",
      "[{'code': 'E11', 'code_system': 'icd10cm', 'description': 'Type 2 diabetes mellitus', 'patient_id': 'PAT001'}]\n",
      "[{'code': 'J45', 'code_system': 'icd10cm', 'description': 'Asthma', 'patient_id': 'PAT002'}]\n"
     ]
    }
   ],
   "source": [
    "# Parse HL7 message and extract codes from segments\n",
    "\n",
    "\n",
    "def extract_patient_id(hl7_text):\n",
    "    \"\"\"Extracts the patient ID from the PID segment (PID-3.1) of the HL7 message.\"\"\"\n",
    "    try:\n",
    "        message = parse_message(hl7_text.replace('\\n', '\\r'), find_groups=False)\n",
    "        for segment in message.children:\n",
    "            if segment.name == 'PID':\n",
    "                # PID-3 may be a repeating field, take the first occurrence\n",
    "                if hasattr(segment, 'pid_3'):\n",
    "                    pid_3 = segment.pid_3\n",
    "                    # If pid_3 is a list (repeating field), take the first\n",
    "                    if isinstance(pid_3, list) and len(pid_3) > 0:\n",
    "                        pid_3 = pid_3[0]\n",
    "                    if hasattr(pid_3, 'cx_1'):\n",
    "                        return pid_3.cx_1.value\n",
    "                    elif hasattr(pid_3, 'value'):\n",
    "                        return pid_3.value\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def extract_loinc_or_labtest_details(hl7_text):\n",
    "    message = parse_message(hl7_text.replace('\\n', '\\r'), find_groups=False)\n",
    "    patient_id = extract_patient_id(hl7_text)\n",
    "    loinc_results = []\n",
    "    for obx in message.children:\n",
    "        if obx.name == 'OBX' and hasattr(obx.obx_3, 'ce_3') and obx.obx_3.ce_3.value in ['LN']:\n",
    "            try:\n",
    "                loinc_code = obx.obx_3.ce_1.value if hasattr(obx.obx_3, 'ce_1') else None\n",
    "                lab_test_name = obx.obx_3.ce_2.value if hasattr(obx.obx_3, 'ce_2') else None\n",
    "                loinc_results.append({\n",
    "                    'code': loinc_code,\n",
    "                    'code_system': 'loinc',\n",
    "                    'description': lab_test_name,\n",
    "                    'patient_id': patient_id\n",
    "                })\n",
    "            except Exception:\n",
    "                continue\n",
    "    return loinc_results\n",
    "\n",
    "\n",
    "def extract_cpt_codes(hl7_text):\n",
    "    message = parse_message(hl7_text.replace('\\n', '\\r'), find_groups=False)\n",
    "    patient_id = extract_patient_id(hl7_text)\n",
    "    cpt_results = []\n",
    "    for segment in message.children:\n",
    "        if segment.name == 'PR1':\n",
    "            try:\n",
    "                cpt_code = segment.pr1_3.ce_1.value if hasattr(segment.pr1_3, 'ce_1') else None\n",
    "                cpt_desc = segment.pr1_3.ce_2.value if hasattr(segment.pr1_3, 'ce_2') else None\n",
    "                if cpt_code or cpt_desc:\n",
    "                    cpt_results.append({\n",
    "                        'code': cpt_code,\n",
    "                        'code_system': 'cpt',\n",
    "                        'description': cpt_desc,\n",
    "                        'patient_id': patient_id})\n",
    "            except Exception:\n",
    "                continue\n",
    "        elif segment.name == 'OBR':\n",
    "            try:\n",
    "                code = segment.obr_4.ce_1.value if hasattr(segment.obr_4, 'ce_1') else None\n",
    "                desc = segment.obr_4.ce_2.value if hasattr(segment.obr_4, 'ce_2') else None\n",
    "                if code and code.isdigit() and (len(code) == 5 or len(code) == 7):\n",
    "                    cpt_results.append({\n",
    "                        'code': code, \n",
    "                        'code_system': 'cpt',\n",
    "                        'description': desc, \n",
    "                        'patient_id': patient_id})\n",
    "            except Exception:\n",
    "                continue\n",
    "    return cpt_results\n",
    "\n",
    "\n",
    "def extract_icd10cm_codes(hl7_text):\n",
    "    message = parse_message(hl7_text.replace('\\n', '\\r'), find_groups=False)\n",
    "    patient_id = extract_patient_id(hl7_text)\n",
    "    icd_results = []\n",
    "    for segment in message.children:\n",
    "        if segment.name == 'DG1':\n",
    "            try:\n",
    "                icd_code = segment.dg1_3.ce_1.value if hasattr(segment.dg1_3, 'ce_1') else (segment.dg1_3.value if hasattr(segment, 'dg1_3') and hasattr(segment.dg1_3, 'value') else None)\n",
    "                icd_desc = segment.dg1_4.value if hasattr(segment, 'dg1_4') and hasattr(segment.dg1_4, 'value') else None\n",
    "                if icd_code or icd_desc:\n",
    "                    icd_results.append({\n",
    "                        'code': icd_code, \n",
    "                        'code_system': 'icd10cm',\n",
    "                        'description': icd_desc, \n",
    "                        'patient_id': patient_id})\n",
    "            except Exception:\n",
    "                continue\n",
    "        elif segment.name == 'OBX':\n",
    "            try:\n",
    "                if hasattr(segment.obx_3, 'ce_3') and segment.obx_3.ce_3.value == 'ICD-10-CM':\n",
    "                    icd_code = segment.obx_3.ce_1.value if hasattr(segment.obx_3, 'ce_1') else None\n",
    "                    icd_desc = segment.obx_3.ce_2.value if hasattr(segment.obx_3, 'ce_2') else None\n",
    "                    if icd_code or icd_desc:\n",
    "                        icd_results.append({\n",
    "                            'code': icd_code, \n",
    "                            'code_system': 'icd10cm',\n",
    "                            'description': icd_desc, \n",
    "                            'patient_id': patient_id})\n",
    "            except Exception:\n",
    "                continue\n",
    "    return icd_results\n",
    "\n",
    "\n",
    "def extract_snomedct_codes(hl7_text):\n",
    "    message = parse_message(hl7_text.replace('\\n', '\\r'), find_groups=False)\n",
    "    patient_id = extract_patient_id(hl7_text)\n",
    "    snomed_results = []\n",
    "    for segment in message.children:\n",
    "        if segment.name in ['OBX', 'OBR', 'PR1']:\n",
    "            try:\n",
    "                if segment.name == 'OBX' and hasattr(segment.obx_3, 'ce_3') and segment.obx_3.ce_3.value in ['SCT', 'SNOMEDCT']:\n",
    "                    code = segment.obx_3.ce_1.value if hasattr(segment.obx_3, 'ce_1') else None\n",
    "                    desc = segment.obx_3.ce_2.value if hasattr(segment.obx_3, 'ce_2') else None\n",
    "                    if code or desc:\n",
    "                        snomed_results.append({\n",
    "                            'code': code,\n",
    "                            'code_system': 'snomedct',\n",
    "                            'description': desc, \n",
    "                            'patient_id': patient_id})\n",
    "                elif segment.name == 'OBR' and hasattr(segment.obr_4, 'ce_3') and segment.obr_4.ce_3.value in ['SCT', 'SNOMEDCT']:\n",
    "                    code = segment.obr_4.ce_1.value if hasattr(segment.obr_4, 'ce_1') else None\n",
    "                    desc = segment.obr_4.ce_2.value if hasattr(segment.obr_4, 'ce_2') else None\n",
    "                    if code or desc:\n",
    "                        snomed_results.append({\n",
    "                            'code': code, \n",
    "                            'code_system' : 'snomedct',\n",
    "                            'description': desc, \n",
    "                            'patient_id': patient_id})\n",
    "                elif segment.name == 'PR1' and hasattr(segment.pr1_4, 'ce_3') and segment.pr1_4.ce_3.value in ['SCT', 'SNOMEDCT']:\n",
    "                    code = segment.pr1_4.ce_1.value if hasattr(segment.pr1_4, 'ce_1') else None\n",
    "                    desc = segment.pr1_4.ce_2.value if hasattr(segment.pr1_4, 'ce_2') else None\n",
    "                    if code or desc:\n",
    "                        snomed_results.append({\n",
    "                            'code': code, \n",
    "                            'code_system' : 'snomedct',\n",
    "                            'description': desc, \n",
    "                            'patient_id': patient_id})\n",
    "            except Exception:\n",
    "                continue\n",
    "    return snomed_results\n",
    "\n",
    "trial_patient_dict = []\n",
    "for hl7_data in hl7_data_dict:\n",
    "    results = extract_loinc_or_labtest_details(hl7_data)\n",
    "    if results:\n",
    "        trial_patient_dict.append(results)\n",
    "        print(results)\n",
    "        continue\n",
    "        \n",
    "    results = extract_cpt_codes(hl7_data)\n",
    "    if results:\n",
    "        trial_patient_dict.append(results)\n",
    "        print(results)\n",
    "        continue\n",
    "        \n",
    "    results = extract_icd10cm_codes(hl7_data)\n",
    "    if results:\n",
    "        trial_patient_dict.append(results)  \n",
    "        print(results)\n",
    "        continue\n",
    "\n",
    "    results = extract_snomedct_codes(hl7_data)\n",
    "    if results:\n",
    "        trial_patient_dict.append(results)\n",
    "        print(results)\n",
    "        continue\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5b77ee",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 5: Code Normalization with IMO API\n",
    "\n",
    "### Normalize Missing or Incomplete Medical Codes\n",
    "\n",
    "This cell enhances patient data by using IMO's normalization API to fill in missing codes:\n",
    "\n",
    "#### Key Functions:\n",
    "\n",
    "1. **`load_config()`**: Loads API credentials from `config.json`\n",
    "2. **`get_auth0_token()`**: Authenticates with IMO's Auth0 service to obtain access token\n",
    "3. **`get_imo_domain()`**: Maps code systems to IMO domain categories:\n",
    "   - `loinc` ‚Üí `lab` (laboratory)\n",
    "   - `icd10cm` ‚Üí `problem` (diagnosis)\n",
    "   - `cpt` ‚Üí `procedure` (procedures)\n",
    "\n",
    "#### Normalization Process:\n",
    "\n",
    "- **Input**: Patient records with missing codes (empty `code` field)\n",
    "- **API Call**: Sends description text to IMO's normalization endpoint\n",
    "- **Enhancement**: Uses LLM-powered transformation and candidate selection\n",
    "- **Output**: Updates `trial_patient_dict` with normalized codes\n",
    "\n",
    "#### API Configuration:\n",
    "- **Endpoint**: `https://api.imohealth.com/precision/normalize`\n",
    "- **Authentication**: Bearer token from Auth0\n",
    "- **Features**: LLM transformation and candidate selection enabled\n",
    "- **Organization**: IMO\n",
    "\n",
    "> **Prerequisites**: Ensure `config.json` contains valid IMO API credentials before running this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfd251a3-7557-4ec6-805e-f647a87c009f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ICD10CM Code Found\n",
      "[{'code': '17968-9', 'code_system': 'loinc', 'description': 'Bacteria6:Prid:Pt:Stool:Nom:Culture', 'patient_id': 'PAT1'}]\n",
      "[{'code': 'I10', 'code_system': 'icd10cm', 'description': 'Essential (primary) hypertension', 'patient_id': 'PAT003'}]\n",
      "[{'code': 'E78.5', 'code_system': 'icd10cm', 'description': 'Hyperlipidemia, unspecified', 'patient_id': 'PAT004'}]\n",
      "[{'code': 'F41.1', 'code_system': 'icd10cm', 'description': 'Generalized anxiety disorder', 'patient_id': 'PAT005'}]\n",
      "[{'code': 'I10', 'code_system': 'icd10cm', 'description': 'Essential (primary) hypertension', 'patient_id': 'PAT2'}]\n",
      "[{'code': '45380', 'code_system': 'cpt', 'description': 'Colonoscopy; with removal of polyp, snare technique', 'patient_id': 'PAT3'}]\n",
      "[{'code': '71260', 'code_system': 'cpt', 'description': 'Thoracic CT Scan with Contrast', 'patient_id': 'PAT4'}]\n",
      "[{'code': '80146002', 'code_system': 'snomedct', 'description': 'POST APPENDECTOMY FOLLOW-UP EXAM', 'patient_id': 'PAT5'}]\n",
      "[{'code': '625-4', 'code_system': 'loinc', 'description': 'Bacteria identified in Stool by Culture', 'patient_id': 'PAT6'}]\n",
      "[{'code': 'I10', 'code_system': 'icd10cm', 'description': 'Systolic hypertension', 'patient_id': 'PAT7'}]\n",
      "[{'code': 'E11', 'code_system': 'icd10cm', 'description': 'Type 2 diabetes mellitus', 'patient_id': 'PAT001'}]\n",
      "[{'code': 'J45', 'code_system': 'icd10cm', 'description': 'Asthma', 'patient_id': 'PAT002'}]\n"
     ]
    }
   ],
   "source": [
    "# Normalize trial patient dictionary if it has bad codes or no codes\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "\n",
    "def load_config():\n",
    "    \"\"\"Load configuration from config.json file\"\"\"\n",
    "    # When running from this notebook (in using-OMOP), config.json is one folder up\n",
    "    if '__file__' in globals():\n",
    "        config_path = os.path.join(os.path.dirname(__file__), 'config.json')\n",
    "    else:\n",
    "        config_path = os.path.join('..', 'config.json')\n",
    "    \n",
    "    with open(config_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def get_auth0_token(config):\n",
    "    \"\"\"Get access token from Auth0\"\"\"\n",
    "    auth0_config = config['auth0']\n",
    "    \n",
    "    payload = {\n",
    "        'client_id': auth0_config['client_id'],\n",
    "        'client_secret': auth0_config['client_secret'],\n",
    "        'audience': auth0_config['audience'],\n",
    "        'grant_type': 'client_credentials'\n",
    "    }\n",
    "    \n",
    "    headers = {\n",
    "        'content-type': 'application/json'\n",
    "    }\n",
    "    \n",
    "    token_url = f\"https://{auth0_config['domain']}/oauth/token\"\n",
    "    \n",
    "    response = requests.post(token_url, json=payload, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        token_data = response.json()\n",
    "        return token_data['access_token']\n",
    "    else:\n",
    "        raise Exception(f\"Failed to get token: {response.status_code} - {response.text}\")\n",
    "\n",
    "# Load configuration and get token\n",
    "config = load_config()\n",
    "Token = get_auth0_token(config)\n",
    "\n",
    "url = \"https://api.imohealth.com/precision/normalize\"\n",
    "\n",
    "def get_imo_domain(code_system):\n",
    "    if code_system == 'loinc':\n",
    "        return 'lab'\n",
    "    if code_system == 'icd10cm':\n",
    "        return 'problem'\n",
    "    if code_system == 'cpt':\n",
    "        return 'procedure'\n",
    "\n",
    "def get_code_by_codesystem(code_system, response_json):\n",
    "    if code_system == 'cpt':\n",
    "     return response_json[\"requests\"][0][\"response\"][\"items\"][0][\"metadata\"][\"mappings\"][\"cpt\"][\"codes\"][0][\"code\"]\n",
    "    elif code_system == 'loinc':\n",
    "     return response_json[\"requests\"][0][\"response\"][\"items\"][0][\"metadata\"][\"mappings\"][\"loinc\"][\"codes\"][0][\"code\"]\n",
    "    elif code_system == 'icd10cm':\n",
    "     print(\"ICD10CM Code Found\")\n",
    "     return response_json[\"requests\"][0][\"response\"][\"items\"][0][\"metadata\"][\"mappings\"][\"icd10cm\"][\"codes\"][0][\"code\"]\n",
    "# POST Normalization patient_trial_dict    \n",
    "for data in trial_patient_dict:\n",
    "    if data[0]['code'] == '':\n",
    "        domain = get_imo_domain(data[0]['code_system'])\n",
    "        \n",
    "        payload = json.dumps({\n",
    "          \"organization_id\": \"IMO\",\n",
    "          \"client_request_id\": \"123\",\n",
    "          \"preferences\": {\n",
    "            \"threshold\": 0,\n",
    "            \"use_llm_transformation\": True,\n",
    "            \"use_llm_select_candidate\": True\n",
    "          },\n",
    "          \"requests\": [\n",
    "            {\n",
    "              \"record_id\": \"10001\",\n",
    "              \"domain\": domain,\n",
    "              \"input_term\": data[0]['description']\n",
    "              \n",
    "            }\n",
    "          ]\n",
    "        })\n",
    "        headers = {\n",
    "          'Content-Type': 'application/json',\n",
    "          'Authorization': f'Bearer {Token}'\n",
    "        }\n",
    "\n",
    "        response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "        response_json = response.json()\n",
    "        if response.status_code == 200:\n",
    "           data[0]['code'] = get_code_by_codesystem(data[0]['code_system'], response_json)\n",
    "\n",
    "# POST Normalization patient_trial_dict\n",
    "for data in trial_patient_dict:\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae9121b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 6: Cohort Criteria Dictionary Search and Download\n",
    "\n",
    "### Interactive Valueset Search and Code Retrieval\n",
    "\n",
    "This cell provides a command-line interface for searching and downloading Cohort criteria valuesets:\n",
    "\n",
    "#### Core Functions:\n",
    "\n",
    "1. **`search_valuesets()`**: Searches IMO's FHIR ValueSet repository using text queries\n",
    "2. **`get_valueset_codes_paged()`**: Downloads complete valueset codes with pagination handling\n",
    "3. **`save_codes_to_csv()`**: Exports downloaded codes to CSV format for analysis\n",
    "4. **`parse_valuesets_from_search()`**: Formats search results into structured data\n",
    "5. **`display_valuesets_table()`**: Shows search results in a formatted table\n",
    "\n",
    "#### Interactive Workflow:\n",
    "\n",
    "1. **Search Query**: Prompts user to enter valueset search term\n",
    "2. **Results Display**: Shows matching valuesets with ID, Name, Scope, and Criteria\n",
    "3. **Selection**: User selects a specific valueset ID to download\n",
    "4. **Download**: Retrieves all codes with progress tracking and pagination\n",
    "5. **Export**: Saves codes to CSV file named `{valueset_id}_codes.csv`\n",
    "\n",
    "#### Technical Features:\n",
    "\n",
    "- **Pagination Support**: Handles large valuesets with automatic page navigation\n",
    "- **Progress Tracking**: Shows download progress for each page\n",
    "- **Error Handling**: Comprehensive error management for API failures\n",
    "- **Global Variables**: Stores results for use in subsequent matching steps\n",
    "\n",
    "#### API Endpoints:\n",
    "- **Search**: `https://api.imohealth.com/fhir/r6/ValueSet/search`\n",
    "- **Retrieve**: `https://api.imohealth.com/fhir/r6/ValueSet/{id}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1dc2f488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Cohort Identification - Dictionary Search and Valueset Selection\n",
      "============================================================\n",
      "‚ùå No search term provided. Exiting.\n"
     ]
    }
   ],
   "source": [
    "# Cohort Dictionary Search and Valueset Selection - Command Line Interface\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "import requests\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def search_valuesets(search_term, token):\n",
    "    \"\"\"Search for valuesets using the IMO API\"\"\"\n",
    "    url = \"https://api.imohealth.com/fhir/r6/ValueSet/search\"\n",
    "    \n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'Authorization': f'Bearer {token}'\n",
    "    }\n",
    "    params = {\"searchText\": search_term}\n",
    "\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        raise Exception(f\"Failed to search valuesets: {response.status_code} - {response.text}\")\n",
    "\n",
    "def get_valueset_codes_paged(valueset_id, token):\n",
    "    \"\"\"Get all codes from a valueset with pagination\"\"\"\n",
    "    all_codes = []\n",
    "    page = 1\n",
    "    page_size = 50\n",
    "    max_pages = 100  # Safety limit to prevent infinite loops\n",
    "    \n",
    "    while page <= max_pages:\n",
    "        url = f\"https://api.imohealth.com/fhir/r6/ValueSet/{valueset_id}\"\n",
    "        \n",
    "        headers = {\n",
    "            'Authorization': f'Bearer {token}'\n",
    "        }\n",
    "        \n",
    "        params = {\n",
    "            'page': page,\n",
    "            'pageSize': page_size\n",
    "        }\n",
    "        \n",
    "        response = requests.get(url, headers=headers, params=params)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"Failed to get valueset codes: {response.status_code} - {response.text}\")\n",
    "        \n",
    "        data = response.json()\n",
    "        expansion = data.get('expansion', {})\n",
    "        total = expansion.get('total', 0)   \n",
    "        codes = expansion.get('contains', [])\n",
    "\n",
    "        # If no codes returned, we've reached the end\n",
    "        if not codes:\n",
    "            break\n",
    "            \n",
    "        all_codes.extend(codes)\n",
    "        print(f\"Retrieved page {page}, codes on this page: {len(codes)}, total codes so far: {len(all_codes)}\")\n",
    "        \n",
    "        # Check if we've retrieved all codes or if this page has fewer than page_size\n",
    "        if len(all_codes) >= total or len(codes) < page_size:\n",
    "            print(f\"Pagination complete. Total codes retrieved: {len(all_codes)}\")\n",
    "            break\n",
    "            \n",
    "        page += 1\n",
    "    \n",
    "    if page > max_pages:\n",
    "        print(f\"‚ö†Ô∏è Reached maximum page limit ({max_pages}). Retrieved {len(all_codes)} codes.\")\n",
    "    \n",
    "    return all_codes\n",
    "\n",
    "def save_codes_to_csv(codes, filename):\n",
    "    \"\"\"Save codes to CSV file in the PythonNotebook folder\"\"\"\n",
    "    filepath = os.path.join(os.path.dirname(__file__) if '__file__' in globals() else '.', filename)\n",
    "    \n",
    "    with open(filepath, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        if codes:\n",
    "            fieldnames = codes[0].keys()\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(codes)\n",
    "    \n",
    "    return filepath\n",
    "\n",
    "def parse_valuesets_from_search(search_results):\n",
    "    \"\"\"Parse and format valueset data from search results\"\"\"\n",
    "    valuesets = []\n",
    "    \n",
    "    if \"entry\" in search_results:\n",
    "        for each_item in search_results.get(\"entry\", []):\n",
    "            resource = each_item.get(\"resource\", {})\n",
    "            description = resource.get(\"description\", \"\")\n",
    "            publisher = resource.get(\"publisher\", \"\")\n",
    "            if publisher == \"IMO\":\n",
    "                continue\n",
    "            \n",
    "            # Extract inclusion and exclusion criteria from description or compose\n",
    "            inclusion_criteria = \"N/A\"\n",
    "            exclusion_criteria = \"N/A\"\n",
    "            # Try to get scope information for better criteria display\n",
    "            scope = resource.get(\"scope\", {})\n",
    "            if scope:\n",
    "                inclusion_criteria = scope.get(\"inclusionCriteria\", [])\n",
    "                exclusion_criteria = scope.get(\"exclusionCriteria\", [])\n",
    "                \n",
    "                \n",
    "            value_set_info = {\n",
    "                \"Id\": resource.get(\"id\"),\n",
    "                \"Name\": resource.get(\"title\", resource.get(\"name\", \"N/A\")),\n",
    "                \"Scope\": description[:100] + \"...\" if len(description) > 100 else description,\n",
    "                \"Inclusion Criteria\": inclusion_criteria,\n",
    "                \"Exclusion Criteria\": exclusion_criteria\n",
    "            }\n",
    "            \n",
    "            valuesets.append(value_set_info)\n",
    "    \n",
    "    return valuesets\n",
    "\n",
    "\n",
    "# Main workflow\n",
    "print(\"üîç Cohort Identification - Dictionary Search and Valueset Selection\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Step 1: Prompt user for valueset name\n",
    "search_term = input(\"Enter the name of the valueset to search for: \").strip()\n",
    "\n",
    "if not search_term:\n",
    "    print(\"‚ùå No search term provided. Exiting.\")\n",
    "else:\n",
    "    try:\n",
    "        # Step 2: Search and display results\n",
    "        print(f\"\\nüîç Searching for valuesets matching '{search_term}'...\")\n",
    "        search_results = search_valuesets(search_term, Token)\n",
    "        valuesets = parse_valuesets_from_search(search_results)\n",
    "        \n",
    "        if valuesets:\n",
    "            #display_valuesets_table(valuesets)\n",
    "            \n",
    "            # Step 3: Prompt user to select valueset ID\n",
    "            print(f\"\\nAvailable valueset IDs:\")\n",
    "            for vs in valuesets:\n",
    "                print(f\"  - {vs['Id']}: {vs['Name']}\")\n",
    "            \n",
    "            selected_id = input(\"\\nEnter the ID of the valueset you would like to download: \").strip()\n",
    "            \n",
    "            # Find the selected valueset\n",
    "            selected_valueset = None\n",
    "            for vs in valuesets:\n",
    "                if vs['Id'] == selected_id:\n",
    "                    selected_valueset = vs\n",
    "                    break\n",
    "            \n",
    "            if selected_valueset:\n",
    "                print(f\"\\nüìã Selected valueset: {selected_valueset['Name']} (ID: {selected_id})\")\n",
    "                \n",
    "                # Step 4: Download the valueset\n",
    "                try:\n",
    "                    print(f\"üíæ Downloading codes for valueset '{selected_valueset['Name']}'...\")\n",
    "                    codes = get_valueset_codes_paged(selected_id, Token)\n",
    "                    \n",
    "                    if codes:\n",
    "                        filename = f\"{selected_id}_codes.csv\"\n",
    "                        filepath = save_codes_to_csv(codes, filename)\n",
    "                        \n",
    "                        print(f\"‚úÖ Successfully downloaded {len(codes)} codes!\")\n",
    "                        print(f\"üìÅ File saved as: {filename}\")\n",
    "                        \n",
    "                        # Store global variables for next steps\n",
    "                        globals()['selected_valueset_info'] = selected_valueset\n",
    "                        globals()['downloaded_codes_file'] = filename\n",
    "                        globals()['downloaded_codes_count'] = len(codes)\n",
    "                        \n",
    "                        print(f\"\\nüéâ Download complete! You can now proceed to the cohort matching step.\")\n",
    "                        \n",
    "                    else:\n",
    "                        print(\"‚ö†Ô∏è No codes found for this valueset.\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Error downloading codes: {str(e)}\")\n",
    "            else:\n",
    "                print(f\"‚ùå Invalid valueset ID '{selected_id}'. Please check the available IDs above.\")\n",
    "                \n",
    "        else:\n",
    "            print(\"‚ùå No valuesets found for your search term.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error searching valuesets: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34dfd60",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 7: Patient-Trial Matching Analysis\n",
    "\n",
    "### Match Patient Codes Against Cohort Eligibility Criteria\n",
    "\n",
    "This cell performs comprehensive patient matching against the downloaded trial dictionary:\n",
    "\n",
    "#### Core Functions:\n",
    "\n",
    "1. **`find_latest_downloaded_csv()`**: Locates the most recent trial dictionary CSV file\n",
    "2. **`load_trial_dictionary()`**: Loads and validates the trial dictionary data\n",
    "3. **`match_patient_codes()`**: Performs code-level matching between patients and trial criteria\n",
    "4. **`display_matching_summary()`**: Generates detailed match statistics and reports\n",
    "\n",
    "#### Matching Process:\n",
    "\n",
    "1. **Dictionary Loading**: Automatically finds and loads the latest downloaded trial CSV\n",
    "2. **Code Alignment**: Maps patient codes to trial dictionary codes with case-insensitive matching\n",
    "3. **Match Detection**: Identifies patients whose codes appear in trial eligibility criteria\n",
    "4. **Result Classification**: Separates patients into matched and unmatched groups\n",
    "5. **Statistical Analysis**: Calculates match percentages and code system breakdowns\n",
    "\n",
    "#### Output Analysis:\n",
    "\n",
    "- **Matched Patients**: Complete details of patients meeting trial criteria\n",
    "- **Unmatched Patients**: Patients not matching current trial eligibility\n",
    "- **Match Statistics**: Percentage calculations and demographic breakdowns\n",
    "- **Code System Analysis**: Breakdown by medical coding systems (ICD-10, CPT, LOINC, SNOMED)\n",
    "\n",
    "#### Data Quality Features:\n",
    "\n",
    "- **Flexible Column Detection**: Automatically identifies code columns in trial dictionary\n",
    "- **Case-Insensitive Matching**: Robust string comparison for reliable matching\n",
    "- **Multiple Match Support**: Handles cases where patient codes match multiple trial entries\n",
    "- **Progress Reporting**: Real-time feedback during matching process\n",
    "\n",
    "#### Global Variables Created:\n",
    "- `matching_results`: Complete results dictionary for further analysis\n",
    "- Contains matched/unmatched patient lists and trial dictionary metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aed1c8f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Patient Code Matching Against Trial Dictionary\n",
      "============================================================\n",
      "‚ùå No downloaded CSV files found. Please run the cohort dictionary search cell first to download a trial dictionary.\n"
     ]
    }
   ],
   "source": [
    "# Patient Code Matching Against Downloaded Trial Dictionary\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def find_latest_downloaded_csv():\n",
    "    \"\"\"Find the most recently downloaded CSV file\"\"\"\n",
    "    csv_files = glob.glob(\"*_codes.csv\")\n",
    "    if not csv_files:\n",
    "        return None\n",
    "    \n",
    "    # Get the most recently modified CSV file\n",
    "    latest_file = max(csv_files, key=os.path.getmtime)\n",
    "    return latest_file\n",
    "\n",
    "def load_trial_dictionary(csv_file):\n",
    "    \"\"\"Load the trial dictionary from CSV file\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file)\n",
    "        print(f\"üìã Loaded trial dictionary from: {csv_file}\")\n",
    "        print(f\"   Total codes in dictionary: {len(df)}\")\n",
    "        \n",
    "        # Display column names to understand the structure\n",
    "        print(f\"   Columns: {list(df.columns)}\")\n",
    "        \n",
    "        # Show first few entries\n",
    "        if len(df) > 0:\n",
    "            print(f\"   Sample entries:\")\n",
    "            print(df.head(3).to_string(index=False))\n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading CSV file: {e}\")\n",
    "        return None\n",
    "\n",
    "def match_patient_codes(trial_patient_dict, trial_df):\n",
    "    \"\"\"Match patient codes against the trial dictionary\"\"\"\n",
    "    if trial_df is None or len(trial_df) == 0:\n",
    "        print(\"‚ùå No trial dictionary data available for matching\")\n",
    "        return []\n",
    "    \n",
    "    # Try to identify the code column in the trial dictionary\n",
    "    code_columns = ['code', 'Code', 'CODE', 'system', 'System']\n",
    "    code_col = None\n",
    "    \n",
    "    for col in code_columns:\n",
    "        if col in trial_df.columns:\n",
    "            code_col = col\n",
    "            break\n",
    "    \n",
    "    if code_col is None:\n",
    "        print(f\"‚ùå Could not identify code column. Available columns: {list(trial_df.columns)}\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"‚úÖ Using '{code_col}' column for code matching\")\n",
    "    \n",
    "    # Convert trial dictionary codes to set for faster lookup\n",
    "    trial_codes = set(trial_df[code_col].astype(str).str.strip().str.upper())\n",
    "    \n",
    "    matched_patients = []\n",
    "    unmatched_patients = []\n",
    "    \n",
    "    print(f\"\\nüîç Matching {len(trial_patient_dict)} patient records against trial dictionary...\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for patient_group in trial_patient_dict:\n",
    "        for patient_record in patient_group:\n",
    "            patient_code = str(patient_record.get('code', '')).strip().upper()\n",
    "            patient_id = patient_record.get('patient_id', 'Unknown')\n",
    "            code_system = patient_record.get('code_system', 'Unknown')\n",
    "            description = patient_record.get('description', 'N/A')\n",
    "            \n",
    "            if patient_code and patient_code in trial_codes:\n",
    "                # Find the matching trial dictionary entry\n",
    "                matching_entries = trial_df[trial_df[code_col].astype(str).str.strip().str.upper() == patient_code]\n",
    "                \n",
    "                match_info = {\n",
    "                    'patient_id': patient_id,\n",
    "                    'patient_code': patient_record.get('code', ''),\n",
    "                    'code_system': code_system,\n",
    "                    'patient_description': description,\n",
    "                    'trial_matches': matching_entries.to_dict('records')\n",
    "                }\n",
    "                matched_patients.append(match_info)\n",
    "                \n",
    "                print(f\"‚úÖ MATCH FOUND!\")\n",
    "                print(f\"   Patient ID: {patient_id}\")\n",
    "                print(f\"   Code: {patient_record.get('code', '')} ({code_system})\")\n",
    "                print(f\"   Description: {description}\")\n",
    "                print(f\"   Trial Dictionary Matches: {len(matching_entries)}\")\n",
    "                \n",
    "                # Show trial dictionary match details\n",
    "                for idx, match in matching_entries.iterrows():\n",
    "                    trial_desc = match.get('display', match.get('Display', match.get('description', 'N/A')))\n",
    "                    print(f\"     - Trial Entry: {trial_desc}\")\n",
    "                print(\"-\" * 60)\n",
    "                \n",
    "            else:\n",
    "                unmatched_info = {\n",
    "                    'patient_id': patient_id,\n",
    "                    'patient_code': patient_record.get('code', ''),\n",
    "                    'code_system': code_system,\n",
    "                    'patient_description': description\n",
    "                }\n",
    "                unmatched_patients.append(unmatched_info)\n",
    "    \n",
    "    return matched_patients, unmatched_patients\n",
    "\n",
    "def display_matching_summary(matched_patients, unmatched_patients):\n",
    "    \"\"\"Display a summary of matching results\"\"\"\n",
    "    total_patients = len(matched_patients) + len(unmatched_patients)\n",
    "    \n",
    "    print(f\"\\nüìä MATCHING SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Total patients analyzed: {total_patients}\")\n",
    "    print(f\"‚úÖ Patients matching trial criteria: {len(matched_patients)}\")\n",
    "    print(f\"‚ùå Patients NOT matching trial criteria: {len(unmatched_patients)}\")\n",
    "    \n",
    "    if total_patients > 0:\n",
    "        match_percentage = (len(matched_patients) / total_patients) * 100\n",
    "        print(f\"üìà Match percentage: {match_percentage:.1f}%\")\n",
    "    \n",
    "    # Group by code system\n",
    "    if matched_patients:\n",
    "        print(f\"\\nüéØ Matched patients by code system:\")\n",
    "        code_systems = {}\n",
    "        for patient in matched_patients:\n",
    "            cs = patient['code_system']\n",
    "            if cs not in code_systems:\n",
    "                code_systems[cs] = 0\n",
    "            code_systems[cs] += 1\n",
    "        \n",
    "        for cs, count in code_systems.items():\n",
    "            print(f\"   {cs}: {count} patients\")\n",
    "    \n",
    "    if unmatched_patients:\n",
    "        print(f\"\\n‚ö†Ô∏è Unmatched patients by code system:\")\n",
    "        code_systems = {}\n",
    "        for patient in unmatched_patients:\n",
    "            cs = patient['code_system']\n",
    "            if cs not in code_systems:\n",
    "                code_systems[cs] = 0\n",
    "            code_systems[cs] += 1\n",
    "        \n",
    "        for cs, count in code_systems.items():\n",
    "            print(f\"   {cs}: {count} patients\")\n",
    "\n",
    "# Main execution\n",
    "print(\"üîç Patient Code Matching Against Trial Dictionary\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check if we have patient data\n",
    "if 'trial_patient_dict' not in globals() or not trial_patient_dict:\n",
    "    print(\"‚ùå No patient data found. Please run the HL7 extraction cells first.\")\n",
    "else:\n",
    "    # Find the latest downloaded CSV file\n",
    "    csv_file = find_latest_downloaded_csv()\n",
    "    \n",
    "    if csv_file:\n",
    "        print(f\"üìÅ Found downloaded trial dictionary: {csv_file}\")\n",
    "        \n",
    "        # Load the trial dictionary\n",
    "        trial_df = load_trial_dictionary(csv_file)\n",
    "        \n",
    "        if trial_df is not None:\n",
    "            # Perform matching\n",
    "            matched_patients, unmatched_patients = match_patient_codes(trial_patient_dict, trial_df)\n",
    "            \n",
    "            # Display summary\n",
    "            display_matching_summary(matched_patients, unmatched_patients)\n",
    "            \n",
    "            # Store results for potential further analysis\n",
    "            matching_results = {\n",
    "                'matched_patients': matched_patients,\n",
    "                'unmatched_patients': unmatched_patients,\n",
    "                'trial_dictionary_file': csv_file,\n",
    "                'total_trial_codes': len(trial_df)\n",
    "            }\n",
    "            \n",
    "            print(f\"\\nüíæ Results stored in 'matching_results' variable for further analysis\")\n",
    "    else:\n",
    "        print(\"‚ùå No downloaded CSV files found. Please run the cohort dictionary search cell first to download a trial dictionary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd826e6a-e3fa-49b6-a596-4f0d3d878fde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
