{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "833934ef",
   "metadata": {},
   "source": [
    "<img src=\"../static/imo_health.png\" alt=\"IMO Health Logo\" width=\"300\"/>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3189cadb",
   "metadata": {},
   "source": [
    "# RWE Cohort Identification - Applying Eligibility Criteria\n",
    "\n",
    "This notebook is the **third step** in the RWE Cohort Identification pipeline. It searches for and downloads clinical trial eligibility criteria valuesets, then matches OMOP patient data against these criteria to identify eligible cohorts.\n",
    "\n",
    "## Workflow Overview\n",
    "\n",
    "This notebook performs cohort matching based on inclusion and exclusion criteria:\n",
    "\n",
    "1. **API Authentication**: Authenticate with IMO's FHIR API services\n",
    "2. **Valueset Search & Download**: Search for and download Cohort eligibility criteria valuesets (both inclusion and exclusion)\n",
    "3. **OMOP Data Loading**: Load patient data in OMOP CDM format from previous step\n",
    "4. **Inclusion Criteria Matching**: Match patients against ALL inclusion criteria lists\n",
    "5. **Exclusion Criteria Filtering**: Remove patients matching ANY exclusion criteria\n",
    "6. **Cohort Summary**: Generate detailed statistics and export final matching results\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- **Previous Steps Completed**:\n",
    "  - Step 1: NLP extraction completed (`patient_output.xlsx` generated)\n",
    "  - Step 2: OMOP conversion completed (`Output/OMOP_CSV/*.csv` files created)\n",
    "- IMO API credentials configured in `config.json`\n",
    "- Required Python packages: `pandas`, `requests`\n",
    "\n",
    "## Example Use Case\n",
    "\n",
    "**Study**: Extract adult patients with Rheumatoid Arthritis on methotrexate\n",
    "\n",
    "**Inclusion Criteria**:\n",
    "- Age ‚â• 18 at index date\n",
    "- ‚â• 2 encounters with ICD-10: M05*, M06* (Rheumatoid arthritis)\n",
    "- At least 1 methotrexate prescription\n",
    "\n",
    "**Exclusion Criteria**:\n",
    "- Any diagnosis of juvenile idiopathic arthritis (M08*)\n",
    "- Missing gender or birthdate\n",
    "\n",
    "---\n",
    "\n",
    "## Step 0: Package Installation and Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80566eeb",
   "metadata": {},
   "source": [
    "### Verify Required Packages\n",
    "\n",
    "This cell verifies that all necessary Python packages are installed:\n",
    "\n",
    "- **`pandas`**: Data manipulation and analysis library\n",
    "- **`openpyxl`**: Reading/writing Excel files\n",
    "- **`xlsxwriter`**: Creating formatted Excel files\n",
    "- **`requests`**: HTTP library for API calls to IMO services\n",
    "\n",
    "> **Note**: These packages should already be installed in your virtual environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e7f5067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in d:\\users\\skale\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: pandas in d:\\users\\skale\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: xlsxwriter in d:\\users\\skale\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.2.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\users\\skale\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\users\\skale\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\users\\skale\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\users\\skale\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (2026.1.4)\n",
      "Requirement already satisfied: numpy>=1.26.0 in d:\\users\\skale\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\users\\skale\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\users\\skale\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\users\\skale\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in d:\\users\\skale\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Install required packages (uncomment if running in a new environment)\n",
    "# Install required packages (uncomment if running in a new environment)\n",
    "%pip install requests pandas xlsxwriter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756084b2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Load API Configuration\n",
    "\n",
    "### Load IMO API Credentials\n",
    "\n",
    "This cell loads the API credentials from `config.json` and obtains an authentication token:\n",
    "\n",
    "- **Auth0 Authentication**: Authenticates with IMO's Auth0 service\n",
    "- **Bearer Token**: Retrieves access token for API calls\n",
    "- **Token Storage**: Stores token globally for use in subsequent steps\n",
    "\n",
    "The configuration file should contain:\n",
    "- `client_id`: Your IMO API client ID\n",
    "- `client_secret`: Your IMO API client secret\n",
    "- `audience`: API audience identifier\n",
    "- `domain`: Auth0 domain\n",
    "\n",
    "This token will be used for both NLP entity extraction and FHIR valueset searches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4064727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully authenticated with IMO API\n",
      "   Token obtained: eyJhbGciOiJSUzI1NiIs...\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def load_config():\n",
    "    \"\"\"Load configuration from config.json file\"\"\"\n",
    "    # When running from this notebook (in using-OMOP), config.json is one folder up\n",
    "    if '__file__' in globals():\n",
    "        config_path = os.path.join(os.path.dirname(__file__), 'config.json')\n",
    "    else:\n",
    "        config_path = os.path.join('..', 'config.json')\n",
    "    \n",
    "    with open(config_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def get_auth0_token(config):\n",
    "    \"\"\"Get access token from Auth0\"\"\"\n",
    "    auth0_config = config['auth0']\n",
    "    \n",
    "    payload = {\n",
    "        'client_id': auth0_config['client_id'],\n",
    "        'client_secret': auth0_config['client_secret'],\n",
    "        'audience': auth0_config['audience'],\n",
    "        'grant_type': 'client_credentials'\n",
    "    }\n",
    "    \n",
    "    headers = {'content-type': 'application/json'}\n",
    "    token_url = f\"https://{auth0_config['domain']}/oauth/token\"\n",
    "    \n",
    "    response = requests.post(token_url, json=payload, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        token_data = response.json()\n",
    "        return token_data['access_token']\n",
    "    else:\n",
    "        raise Exception(f\"Failed to get token: {response.status_code} - {response.text}\")\n",
    "\n",
    "# Load configuration and get token\n",
    "config = load_config()\n",
    "Token = get_auth0_token(config)\n",
    "\n",
    "print(\"‚úÖ Successfully authenticated with IMO API\")\n",
    "print(f\"   Token obtained: {Token[:20]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d76f5c0",
   "metadata": {},
   "source": [
    "## Step 2 Searching and Download Valusets created for the following RWE study\n",
    "\n",
    "Scope: Extract adult patients over the age of 18 with Rheumatoid Artheritis on methotrexate \n",
    "1.\tInclusion:\n",
    "2.\t  - Age ‚â• 18 at index date\n",
    "3.\t  - ‚â• 2 encounters with ICD-10: M05*, M06* (Rheumatoid arthritis)\n",
    "4.\t  - At least 1 methotrexate prescription \n",
    "\n",
    "5.\t\n",
    "6.\tExclusion:\n",
    "7.\t  - Any diagnosis of juvenile idiopathic arthritis (M08*)\n",
    "8.\t  - Missing gender or birthdate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dc2f488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Cohort Dictionary Search and Valueset Selection\n",
      "============================================================\n",
      "Search for RWE Rheumatoid arthritis valuesets\n",
      "\n",
      "üîç Searching for valuesets matching 'RWE Rheumatoid arthritis-Condition To Include'...\n",
      "\n",
      "Available valueset IDs:\n",
      "  - 18950: RWE Rheumatoid arthritis-Conditions To Include updated\n",
      "\n",
      "üìã Selected valueset: RWE Rheumatoid arthritis-Conditions To Include updated (ID: 18950)\n",
      "üíæ Downloading codes for valueset 'RWE Rheumatoid arthritis-Conditions To Include updated'...\n",
      "Retrieved page 1, codes on this page: 50, total codes so far: 50\n",
      "Retrieved page 2, codes on this page: 50, total codes so far: 100\n",
      "Retrieved page 3, codes on this page: 50, total codes so far: 150\n",
      "Retrieved page 4, codes on this page: 50, total codes so far: 200\n",
      "Retrieved page 5, codes on this page: 50, total codes so far: 250\n",
      "Retrieved page 6, codes on this page: 50, total codes so far: 300\n",
      "Retrieved page 7, codes on this page: 50, total codes so far: 350\n",
      "Retrieved page 8, codes on this page: 50, total codes so far: 400\n",
      "Retrieved page 9, codes on this page: 50, total codes so far: 450\n",
      "Retrieved page 10, codes on this page: 50, total codes so far: 500\n",
      "Retrieved page 11, codes on this page: 50, total codes so far: 550\n",
      "Retrieved page 12, codes on this page: 50, total codes so far: 600\n",
      "Retrieved page 13, codes on this page: 50, total codes so far: 650\n",
      "Retrieved page 14, codes on this page: 50, total codes so far: 700\n",
      "Retrieved page 15, codes on this page: 50, total codes so far: 750\n",
      "Retrieved page 16, codes on this page: 50, total codes so far: 800\n",
      "Retrieved page 17, codes on this page: 50, total codes so far: 850\n",
      "Retrieved page 18, codes on this page: 50, total codes so far: 900\n",
      "Retrieved page 19, codes on this page: 50, total codes so far: 950\n",
      "Retrieved page 20, codes on this page: 50, total codes so far: 1000\n",
      "Retrieved page 21, codes on this page: 50, total codes so far: 1050\n",
      "Retrieved page 22, codes on this page: 50, total codes so far: 1100\n",
      "Retrieved page 23, codes on this page: 50, total codes so far: 1150\n",
      "Retrieved page 24, codes on this page: 50, total codes so far: 1200\n",
      "Retrieved page 25, codes on this page: 50, total codes so far: 1250\n",
      "Retrieved page 26, codes on this page: 50, total codes so far: 1300\n",
      "Retrieved page 27, codes on this page: 50, total codes so far: 1350\n",
      "Retrieved page 28, codes on this page: 50, total codes so far: 1400\n",
      "Retrieved page 29, codes on this page: 50, total codes so far: 1450\n",
      "Retrieved page 30, codes on this page: 50, total codes so far: 1500\n",
      "Retrieved page 31, codes on this page: 50, total codes so far: 1550\n",
      "Retrieved page 32, codes on this page: 50, total codes so far: 1600\n",
      "Retrieved page 33, codes on this page: 50, total codes so far: 1650\n",
      "Retrieved page 34, codes on this page: 50, total codes so far: 1700\n",
      "Retrieved page 35, codes on this page: 50, total codes so far: 1750\n",
      "Retrieved page 36, codes on this page: 50, total codes so far: 1800\n",
      "Retrieved page 37, codes on this page: 50, total codes so far: 1850\n",
      "Retrieved page 38, codes on this page: 50, total codes so far: 1900\n",
      "Retrieved page 39, codes on this page: 50, total codes so far: 1950\n",
      "Retrieved page 40, codes on this page: 50, total codes so far: 2000\n",
      "Retrieved page 41, codes on this page: 50, total codes so far: 2050\n",
      "Retrieved page 42, codes on this page: 50, total codes so far: 2100\n",
      "Retrieved page 43, codes on this page: 50, total codes so far: 2150\n",
      "Retrieved page 44, codes on this page: 50, total codes so far: 2200\n",
      "Retrieved page 45, codes on this page: 50, total codes so far: 2250\n",
      "Retrieved page 46, codes on this page: 50, total codes so far: 2300\n",
      "Retrieved page 47, codes on this page: 50, total codes so far: 2350\n",
      "Retrieved page 48, codes on this page: 50, total codes so far: 2400\n",
      "Retrieved page 49, codes on this page: 50, total codes so far: 2450\n",
      "Retrieved page 50, codes on this page: 50, total codes so far: 2500\n",
      "Retrieved page 51, codes on this page: 50, total codes so far: 2550\n",
      "Retrieved page 52, codes on this page: 34, total codes so far: 2584\n",
      "Pagination complete. Total codes retrieved: 2584\n",
      "‚úÖ Successfully downloaded 2584 codes!\n",
      "üìÅ File saved as: Inclusion_18950_codes.csv\n",
      "\n",
      "üéâ Download complete! You can now proceed to the patient matching step.\n",
      "\n",
      "üîç Searching for valuesets matching 'RWE Rheumatoid arthritis-Medications To Include'...\n",
      "\n",
      "Available valueset IDs:\n",
      "  - 18957: RWE Rheumatoid arthritis-Medications To Include\n",
      "\n",
      "üìã Selected valueset: RWE Rheumatoid arthritis-Medications To Include (ID: 18957)\n",
      "üíæ Downloading codes for valueset 'RWE Rheumatoid arthritis-Medications To Include'...\n",
      "Retrieved page 1, codes on this page: 50, total codes so far: 50\n",
      "Retrieved page 2, codes on this page: 50, total codes so far: 100\n",
      "Retrieved page 3, codes on this page: 50, total codes so far: 150\n",
      "Retrieved page 4, codes on this page: 50, total codes so far: 200\n",
      "Retrieved page 5, codes on this page: 12, total codes so far: 212\n",
      "Pagination complete. Total codes retrieved: 212\n",
      "‚úÖ Successfully downloaded 212 codes!\n",
      "üìÅ File saved as: Inclusion_18957_codes.csv\n",
      "\n",
      "üéâ Download complete! You can now proceed to the patient matching step.\n",
      "\n",
      "üîç Searching for valuesets matching 'RWE Rheumatoid arthritis-Condition To Exclude'...\n",
      "\n",
      "Available valueset IDs:\n",
      "  - 19150: RWE Rheumatoid arthritis-Conditions To Exclude\n",
      "\n",
      "üìã Selected valueset: RWE Rheumatoid arthritis-Conditions To Exclude (ID: 19150)\n",
      "üíæ Downloading codes for valueset 'RWE Rheumatoid arthritis-Conditions To Exclude'...\n",
      "Retrieved page 1, codes on this page: 50, total codes so far: 50\n",
      "Retrieved page 2, codes on this page: 50, total codes so far: 100\n",
      "Retrieved page 3, codes on this page: 50, total codes so far: 150\n",
      "Retrieved page 4, codes on this page: 50, total codes so far: 200\n",
      "Retrieved page 5, codes on this page: 50, total codes so far: 250\n",
      "Retrieved page 6, codes on this page: 50, total codes so far: 300\n",
      "Retrieved page 7, codes on this page: 50, total codes so far: 350\n",
      "Retrieved page 8, codes on this page: 50, total codes so far: 400\n",
      "Retrieved page 9, codes on this page: 50, total codes so far: 450\n",
      "Retrieved page 10, codes on this page: 50, total codes so far: 500\n",
      "Retrieved page 11, codes on this page: 50, total codes so far: 550\n",
      "Retrieved page 12, codes on this page: 50, total codes so far: 600\n",
      "Retrieved page 13, codes on this page: 50, total codes so far: 650\n",
      "Retrieved page 14, codes on this page: 50, total codes so far: 700\n",
      "Retrieved page 15, codes on this page: 50, total codes so far: 750\n",
      "Retrieved page 16, codes on this page: 50, total codes so far: 800\n",
      "Retrieved page 17, codes on this page: 50, total codes so far: 850\n",
      "Retrieved page 18, codes on this page: 50, total codes so far: 900\n",
      "Retrieved page 19, codes on this page: 50, total codes so far: 950\n",
      "Retrieved page 20, codes on this page: 50, total codes so far: 1000\n",
      "Retrieved page 21, codes on this page: 50, total codes so far: 1050\n",
      "Retrieved page 22, codes on this page: 50, total codes so far: 1100\n",
      "Retrieved page 23, codes on this page: 6, total codes so far: 1106\n",
      "Pagination complete. Total codes retrieved: 1106\n",
      "‚úÖ Successfully downloaded 1106 codes!\n",
      "üìÅ File saved as: Exclusion_19150_codes.csv\n",
      "\n",
      "üéâ Download complete! You can now proceed to the patient matching step.\n"
     ]
    }
   ],
   "source": [
    "# Cohort Dictionary Search and Valueset Selection - Command Line Interface\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "import requests\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def search_valuesets(search_term, token):\n",
    "    \"\"\"Search for valuesets using the IMO API\"\"\"\n",
    "    url = \"https://api.imohealth.com/fhir/r6/ValueSet/search\"\n",
    "    \n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'Authorization': f'Bearer {token}'\n",
    "    }\n",
    "    params = {\"searchText\": search_term}\n",
    "\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        raise Exception(f\"Failed to search valuesets: {response.status_code} - {response.text}\")\n",
    "\n",
    "def get_valueset_codes_paged(valueset_id, token):\n",
    "    \"\"\"Get all codes from a valueset with pagination\"\"\"\n",
    "    all_codes = []\n",
    "    page = 1\n",
    "    page_size = 50\n",
    "    max_pages = 100  # Safety limit to prevent infinite loops\n",
    "    \n",
    "    while page <= max_pages:\n",
    "        url = f\"https://api.imohealth.com/fhir/r6/ValueSet/{valueset_id}\"\n",
    "        \n",
    "        headers = {\n",
    "            'Authorization': f'Bearer {token}'\n",
    "        }\n",
    "        \n",
    "        params = {\n",
    "            'page': page,\n",
    "            'pageSize': page_size\n",
    "        }\n",
    "        \n",
    "        response = requests.get(url, headers=headers, params=params)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"Failed to get valueset codes: {response.status_code} - {response.text}\")\n",
    "        \n",
    "        data = response.json()\n",
    "        expansion = data.get('expansion', {})\n",
    "        total = expansion.get('total', 0)   \n",
    "        codes = expansion.get('contains', [])\n",
    "\n",
    "        # If no codes returned, we've reached the end\n",
    "        if not codes:\n",
    "            break\n",
    "            \n",
    "        all_codes.extend(codes)\n",
    "        print(f\"Retrieved page {page}, codes on this page: {len(codes)}, total codes so far: {len(all_codes)}\")\n",
    "        \n",
    "        # Check if we've retrieved all codes or if this page has fewer than page_size\n",
    "        if len(all_codes) >= total or len(codes) < page_size:\n",
    "            print(f\"Pagination complete. Total codes retrieved: {len(all_codes)}\")\n",
    "            break\n",
    "            \n",
    "        page += 1\n",
    "    \n",
    "    if page > max_pages:\n",
    "        print(f\"‚ö†Ô∏è Reached maximum page limit ({max_pages}). Retrieved {len(all_codes)} codes.\")\n",
    "    \n",
    "    return all_codes\n",
    "\n",
    "def save_codes_to_csv(codes, filename):\n",
    "    \"\"\"Save codes to CSV file in the PythonNotebook folder\"\"\"\n",
    "    filepath = os.path.join(os.path.dirname(__file__) if '__file__' in globals() else '.', filename)\n",
    "    \n",
    "    with open(filepath, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        if codes:\n",
    "            fieldnames = codes[0].keys()\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(codes)\n",
    "    \n",
    "    return filepath\n",
    "\n",
    "def parse_valuesets_from_search(search_results):\n",
    "    \"\"\"Parse and format valueset data from search results\"\"\"\n",
    "    valuesets = []\n",
    "    \n",
    "    if \"entry\" in search_results:\n",
    "        for each_item in search_results.get(\"entry\", []):\n",
    "            resource = each_item.get(\"resource\", {})\n",
    "            description = resource.get(\"description\", \"\")\n",
    "            publisher = resource.get(\"publisher\", \"\")\n",
    "            if publisher == \"IMO\":\n",
    "                continue\n",
    "            \n",
    "            # Extract inclusion and exclusion criteria from description or compose\n",
    "            inclusion_criteria = \"N/A\"\n",
    "            exclusion_criteria = \"N/A\"\n",
    "            # Try to get scope information for better criteria display\n",
    "            scope = resource.get(\"scope\", {})\n",
    "            if scope:\n",
    "                inclusion_criteria = scope.get(\"inclusionCriteria\", [])\n",
    "                exclusion_criteria = scope.get(\"exclusionCriteria\", [])\n",
    "                \n",
    "                \n",
    "            value_set_info = {\n",
    "                \"Id\": resource.get(\"id\"),\n",
    "                \"Name\": resource.get(\"title\", resource.get(\"name\", \"N/A\")),\n",
    "                \"Scope\": description[:100] + \"...\" if len(description) > 100 else description,\n",
    "                \"Inclusion Criteria\": inclusion_criteria,\n",
    "                \"Exclusion Criteria\": exclusion_criteria\n",
    "            }\n",
    "            \n",
    "            valuesets.append(value_set_info)\n",
    "    \n",
    "    return valuesets\n",
    "\n",
    "\n",
    "# Main workflow\n",
    "print(\"üîç Cohort Dictionary Search and Valueset Selection\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Step 1: Prompt user for valueset name\n",
    "print(\"Search for RWE Rheumatoid arthritis valuesets\")\n",
    "search_inclusion_criteria = [\"RWE Rheumatoid arthritis-Condition To Include\", \"RWE Rheumatoid arthritis-Medications To Include\",]\n",
    "search_exlusion_critera = [\"RWE Rheumatoid arthritis-Condition To Exclude\"]\n",
    "\n",
    "for search_term in search_inclusion_criteria + search_exlusion_critera:\n",
    "    if not search_term:\n",
    "        print(\"‚ùå No search term provided. Exiting.\")\n",
    "    else:\n",
    "        if search_term in search_inclusion_criteria:\n",
    "            prefix = \"Inclusion_\"\n",
    "        else:\n",
    "            prefix = \"Exclusion_\"\n",
    "        try:\n",
    "            # Step 2: Search and display results\n",
    "            print(f\"\\nüîç Searching for valuesets matching '{search_term}'...\")\n",
    "            search_results = search_valuesets(search_term, Token)\n",
    "            valuesets = parse_valuesets_from_search(search_results)\n",
    "            \n",
    "            if valuesets:\n",
    "                #display_valuesets_table(valuesets)\n",
    "                \n",
    "                # Step 3: Prompt user to select valueset ID\n",
    "                print(f\"\\nAvailable valueset IDs:\")\n",
    "                for vs in valuesets:\n",
    "                    print(f\"  - {vs['Id']}: {vs['Name']}\")\n",
    "                    selected_valueset = vs\n",
    "                    selected_id = vs['Id']\n",
    "                    \n",
    "                    if selected_valueset:\n",
    "                        print(f\"\\nüìã Selected valueset: {selected_valueset['Name']} (ID: {selected_id})\")\n",
    "                        \n",
    "                        # Step 4: Download the valueset\n",
    "                        try:\n",
    "                            print(f\"üíæ Downloading codes for valueset '{selected_valueset['Name']}'...\")\n",
    "                            codes = get_valueset_codes_paged(selected_id, Token)\n",
    "                            \n",
    "                            if codes:\n",
    "                                filename = prefix + f\"{selected_id}_codes.csv\"\n",
    "                                filepath = save_codes_to_csv(codes, filename)\n",
    "                                \n",
    "                                print(f\"‚úÖ Successfully downloaded {len(codes)} codes!\")\n",
    "                                print(f\"üìÅ File saved as: {filename}\")\n",
    "                                \n",
    "                                # Store global variables for next steps\n",
    "                                globals()['selected_valueset_info'] = selected_valueset\n",
    "                                globals()['downloaded_codes_file'] = filename\n",
    "                                globals()['downloaded_codes_count'] = len(codes)\n",
    "                                \n",
    "                                print(f\"\\nüéâ Download complete! You can now proceed to the patient matching step.\")\n",
    "                                \n",
    "                            else:\n",
    "                                print(\"‚ö†Ô∏è No codes found for this valueset.\")\n",
    "                                \n",
    "                        except Exception as e:\n",
    "                            print(f\"‚ùå Error downloading codes: {str(e)}\")\n",
    "                    else:\n",
    "                        print(f\"‚ùå Invalid valueset ID '{selected_id}'. Please check the available IDs above.\")\n",
    "                        \n",
    "            else:\n",
    "                print(\"‚ùå No valuesets found for your search term.\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error searching valuesets: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34dfd60",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Patient Cohort Matching Against Inclusion Criteria\n",
    "\n",
    "### Match OMOP Patient Data Against Downloaded Eligibility Criteria\n",
    "\n",
    "This section performs comprehensive cohort matching between OMOP patient data and inclusion criteria valuesets.\n",
    "\n",
    "#### Core Functions:\n",
    "\n",
    "1. **`find_latest_inclusion_csvs()`**: Locates all downloaded inclusion criteria CSV files\n",
    "2. **`load_omop_data()`**: Loads OMOP clinical tables from CSV files\n",
    "3. **`match_omop_codes()`**: Matches patients against ALL inclusion criteria lists\n",
    "\n",
    "#### Matching Logic:\n",
    "\n",
    "**Important**: A patient is only included in the final cohort if they match **at least one code** in **EVERY** inclusion criteria list.\n",
    "\n",
    "For example, if you have:\n",
    "- Inclusion List 1: Rheumatoid arthritis conditions (M05*, M06*)\n",
    "- Inclusion List 2: Methotrexate medications\n",
    "\n",
    "A patient must have:\n",
    "- At least 1 match in List 1 (RA diagnosis) **AND**\n",
    "- At least 1 match in List 2 (Methotrexate prescription)\n",
    "\n",
    "Patients matching only some of the inclusion lists are excluded.\n",
    "\n",
    "#### OMOP Tables Processed:\n",
    "\n",
    "- **CONDITION_OCCURRENCE**: Diagnoses/problems (ICD10CM codes)\n",
    "- **DRUG_EXPOSURE**: Medications (RXNORM codes)\n",
    "- **MEASUREMENT**: Lab tests (LOINC codes)\n",
    "- **PROCEDURE_OCCURRENCE**: Procedures (CPT codes)\n",
    "\n",
    "#### Output:\n",
    "\n",
    "- **Console**: Detailed matching progress with patient IDs and matched codes\n",
    "- **CSV Export**: `Output/cohort_matching_results.csv` with matched patient details\n",
    "- **Global Variable**: `cohort_matching_results` dictionary for further analysis\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aed1c8f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç OMOP-Based Patient Cohort Matching Against Cohort Dictionary\n",
      "================================================================================\n",
      "üìÅ Found 2 inclusion trial dictionaries:\n",
      "   - Inclusion_18957_codes.csv\n",
      "   - Inclusion_18950_codes.csv\n",
      "   ‚úÖ Loaded Inclusion_18957_codes.csv: 212 codes\n",
      "   ‚úÖ Loaded Inclusion_18950_codes.csv: 2584 codes\n",
      "üìä Loading OMOP clinical tables from: Output/OMOP_CSV\n",
      "   ‚úÖ CONDITION_OCCURRENCE: 712 records\n",
      "   ‚úÖ DRUG_EXPOSURE: 262 records\n",
      "   ‚úÖ MEASUREMENT: 646 records\n",
      "   ‚úÖ PROCEDURE_OCCURRENCE: 59 records\n",
      "\n",
      "üîç Matching OMOP data against each inclusion trial dictionary...\n",
      "================================================================================\n",
      "\n",
      "Processing OMOP table: CONDITION_OCCURRENCE with 712 records\n",
      "   Match found - Patient ID: 1104223764212704475, Code: M06.9, Inclusion List: 2\n",
      "   Match found - Patient ID: 443887166406931539, Code: M06.9, Inclusion List: 2\n",
      "\n",
      "Processing OMOP table: DRUG_EXPOSURE with 262 records\n",
      "   Match found - Patient ID: 443887166406931539, Code: 328406, Inclusion List: 1\n",
      "\n",
      "Processing OMOP table: MEASUREMENT with 646 records\n",
      "\n",
      "Processing OMOP table: PROCEDURE_OCCURRENCE with 59 records\n",
      "\n",
      "================================================================================\n",
      "üë• Patients matching ALL inclusion criteria: 1\n",
      "\n",
      "‚úÖ Matching complete. Found 1 eligible patients.\n",
      "   Proceed to next cell for detailed cohort summary.\n"
     ]
    }
   ],
   "source": [
    "# Patient Cohort Matching Against Trial Dictionary using OMOP Data\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from collections import defaultdict\n",
    "\n",
    "def find_latest_inclusion_csvs():\n",
    "    \"\"\"Find all latest downloaded trial dictionary CSV files that start with 'Inclusion'\"\"\"\n",
    "    csv_files = glob.glob(\"Inclusion*_codes.csv\")\n",
    "    if not csv_files:\n",
    "        return []\n",
    "    # Sort by modification time, newest first\n",
    "    csv_files = sorted(csv_files, key=os.path.getmtime, reverse=True)\n",
    "    return csv_files\n",
    "\n",
    "def load_omop_data(omop_dir='Output/OMOP_CSV'):\n",
    "    \"\"\"Load OMOP clinical tables from CSV files\"\"\"\n",
    "    omop_tables = {}\n",
    "    \n",
    "    if not os.path.exists(omop_dir):\n",
    "        print(f\"‚ùå OMOP directory not found: {omop_dir}\")\n",
    "        return omop_tables\n",
    "    \n",
    "    # Define the clinical tables to load\n",
    "    clinical_tables = {\n",
    "        'CONDITION_OCCURRENCE': 'condition_source_value',\n",
    "        'DRUG_EXPOSURE': 'drug_source_value',\n",
    "        'MEASUREMENT': 'measurement_source_value',\n",
    "        'PROCEDURE_OCCURRENCE': 'procedure_source_value'\n",
    "    }\n",
    "    \n",
    "    print(f\"üìä Loading OMOP clinical tables from: {omop_dir}\")\n",
    "    \n",
    "    for table_name, code_column in clinical_tables.items():\n",
    "        file_path = os.path.join(omop_dir, f'{table_name}.csv')\n",
    "        \n",
    "        if os.path.exists(file_path):\n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "                omop_tables[table_name] = df\n",
    "                print(f\"   ‚úÖ {table_name}: {len(df)} records\")\n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Error loading {table_name}: {e}\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è {table_name}.csv not found\")\n",
    "    \n",
    "    return omop_tables\n",
    "\n",
    "def match_omop_codes(omop_tables, inclusion_trial_dfs):\n",
    "    \"\"\"\n",
    "    Match OMOP source codes against each inclusion trial dictionary.\n",
    "    Only patients who match at least one code in every inclusion dictionary are returned.\n",
    "    \"\"\"\n",
    "    if not inclusion_trial_dfs or any(df is None or len(df) == 0 for df in inclusion_trial_dfs):\n",
    "        print(\"‚ùå No valid inclusion trial dictionary data available for matching\")\n",
    "        return {}\n",
    "\n",
    "    # Prepare code sets for each inclusion dictionary\n",
    "    code_columns = ['code', 'Code', 'CODE', 'system', 'System']\n",
    "    inclusion_code_sets = []\n",
    "    inclusion_code_cols = []\n",
    "    for trial_df in inclusion_trial_dfs:\n",
    "        code_col = next((col for col in code_columns if col in trial_df.columns), None)\n",
    "        if code_col is None:\n",
    "            print(f\"‚ùå Could not identify code column. Available columns: {list(trial_df.columns)}\")\n",
    "            return {}\n",
    "        inclusion_code_cols.append(code_col)\n",
    "        inclusion_code_sets.append(set(trial_df[code_col].astype(str).str.strip().str.upper()))\n",
    "\n",
    "    # Track matches by patient for each inclusion list\n",
    "    patient_matches_per_inclusion = [defaultdict(int) for _ in inclusion_trial_dfs]\n",
    "\n",
    "    omop_code_columns = {\n",
    "        'CONDITION_OCCURRENCE': 'condition_source_value',\n",
    "        'DRUG_EXPOSURE': 'drug_source_value',\n",
    "        'MEASUREMENT': 'measurement_source_value',\n",
    "        'PROCEDURE_OCCURRENCE': 'procedure_source_value'\n",
    "    }\n",
    "\n",
    "    print(f\"\\nüîç Matching OMOP data against each inclusion trial dictionary...\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    for table_name, df in omop_tables.items():\n",
    "        code_column = omop_code_columns.get(table_name)\n",
    "        if code_column not in df.columns or 'person_id' not in df.columns:\n",
    "            continue\n",
    "        print(f\"\\nProcessing OMOP table: {table_name} with {len(df)} records\")\n",
    "        for idx, row in df.iterrows():\n",
    "            source_code = str(row[code_column]).strip().upper()\n",
    "            person_id = row['person_id']\n",
    "            for i, code_set in enumerate(inclusion_code_sets):\n",
    "                if source_code in code_set:\n",
    "                    print(f\"   Match found - Patient ID: {person_id}, Code: {source_code}, Inclusion List: {i+1}\")\n",
    "                    patient_matches_per_inclusion[i][person_id] += 1\n",
    "\n",
    "    # Find patients present in all inclusion lists\n",
    "    matched_patients = set(patient_matches_per_inclusion[0].keys())\n",
    "    for matches in patient_matches_per_inclusion[1:]:\n",
    "        matched_patients &= set(matches.keys())\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üë• Patients matching ALL inclusion criteria: {len(matched_patients)}\")\n",
    "\n",
    "    # Collect detailed match info for each patient\n",
    "    final_matches = {}\n",
    "    for person_id in matched_patients:\n",
    "        match_info = {\n",
    "            'person_id': person_id,\n",
    "            'inclusion_match_counts': [matches[person_id] for matches in patient_matches_per_inclusion],\n",
    "            'total_inclusion_lists': len(inclusion_trial_dfs)\n",
    "        }\n",
    "        final_matches[person_id] = match_info\n",
    "\n",
    "    return final_matches\n",
    "\n",
    "# Main execution - Load and Match\n",
    "print(\"üîç OMOP-Based Patient Cohort Matching Against Cohort Dictionary\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Find and load all inclusion trial dictionaries\n",
    "inclusion_csv_files = find_latest_inclusion_csvs()\n",
    "\n",
    "if inclusion_csv_files:\n",
    "    print(f\"üìÅ Found {len(inclusion_csv_files)} inclusion trial dictionaries:\")\n",
    "    for f in inclusion_csv_files:\n",
    "        print(f\"   - {f}\")\n",
    "\n",
    "    # Load all inclusion trial dictionaries into DataFrames\n",
    "    inclusion_trial_dfs = []\n",
    "    for csv_file in inclusion_csv_files:\n",
    "        try:\n",
    "            df = pd.read_csv(csv_file)\n",
    "            inclusion_trial_dfs.append(df)\n",
    "            print(f\"   ‚úÖ Loaded {csv_file}: {len(df)} codes\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error loading {csv_file}: {e}\")\n",
    "\n",
    "    # Load OMOP data\n",
    "    omop_tables = load_omop_data()\n",
    "\n",
    "    if omop_tables and inclusion_trial_dfs:\n",
    "        # Perform matching: find patients who match at least one code in every inclusion dictionary\n",
    "        patient_matches = match_omop_codes(omop_tables, inclusion_trial_dfs)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Matching complete. Found {len(patient_matches)} eligible patients.\")\n",
    "        print(f\"   Proceed to next cell for detailed cohort summary.\")\n",
    "    else:\n",
    "        patient_matches = {}\n",
    "        print(\"‚ùå No OMOP data tables loaded or no valid inclusion trial dictionaries found.\")\n",
    "else:\n",
    "    patient_matches = {}\n",
    "    omop_tables = {}\n",
    "    inclusion_csv_files = []\n",
    "    print(\"‚ùå No inclusion trial dictionary CSV files found.\")\n",
    "    print(\"   Please run Step 2 first to search and download trial dictionaries.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9882ed57",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3.5: Apply Exclusion Criteria Filtering\n",
    "\n",
    "### Remove Patients Matching Exclusion Criteria\n",
    "\n",
    "This section filters out patients who matched the inclusion criteria but also have any conditions, medications, or procedures listed in the exclusion criteria.\n",
    "\n",
    "#### Exclusion Logic:\n",
    "\n",
    "**Important**: A patient is excluded from the final cohort if they match **any code** in **any** exclusion criteria list.\n",
    "\n",
    "For example, if your exclusion criteria include:\n",
    "- Exclusion List 1: Juvenile idiopathic arthritis (M08*)\n",
    "- Exclusion List 2: Pregnancy-related conditions\n",
    "\n",
    "A patient will be excluded if they have:\n",
    "- Any match in List 1 **OR**\n",
    "- Any match in List 2\n",
    "\n",
    "This is the opposite of inclusion logic (which requires matches in ALL lists).\n",
    "\n",
    "#### Process:\n",
    "\n",
    "1. **Find Exclusion CSV Files**: Locate all `Exclusion*_codes.csv` files\n",
    "2. **Match Against OMOP Data**: Check if inclusion-matched patients have any exclusion codes\n",
    "3. **Filter Cohort**: Remove any patient with exclusion matches\n",
    "4. **Report**: Display how many patients were excluded and why\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d519a32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üö´ EXCLUSION CRITERIA FILTERING\n",
      "================================================================================\n",
      "üìÅ Found 1 exclusion trial dictionaries:\n",
      "   - Exclusion_19150_codes.csv\n",
      "   ‚úÖ Loaded Exclusion_19150_codes.csv: 1106 codes\n",
      "\n",
      "üîç Checking inclusion-matched patients against exclusion criteria...\n",
      "================================================================================\n",
      "\n",
      "Processing OMOP table: CONDITION_OCCURRENCE\n",
      "\n",
      "Processing OMOP table: DRUG_EXPOSURE\n",
      "\n",
      "Processing OMOP table: MEASUREMENT\n",
      "\n",
      "Processing OMOP table: PROCEDURE_OCCURRENCE\n",
      "\n",
      "‚úÖ No patients matched exclusion criteria. All 1 patients remain in cohort.\n",
      "\n",
      "‚úÖ Exclusion filtering complete. Proceed to next cell for cohort summary.\n"
     ]
    }
   ],
   "source": [
    "# Apply Exclusion Criteria Filtering\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from collections import defaultdict\n",
    "\n",
    "def find_latest_exclusion_csvs():\n",
    "    \"\"\"Find all downloaded exclusion dictionary CSV files that start with 'Exclusion'\"\"\"\n",
    "    csv_files = glob.glob(\"Exclusion*_codes.csv\")\n",
    "    if not csv_files:\n",
    "        return []\n",
    "    # Sort by modification time, newest first\n",
    "    csv_files = sorted(csv_files, key=os.path.getmtime, reverse=True)\n",
    "    return csv_files\n",
    "\n",
    "def match_exclusion_codes(omop_tables, exclusion_trial_dfs, inclusion_matched_patients):\n",
    "    \"\"\"\n",
    "    Check if any inclusion-matched patients have exclusion criteria codes.\n",
    "    Returns a set of person_ids that should be excluded.\n",
    "    \"\"\"\n",
    "    if not exclusion_trial_dfs or any(df is None or len(df) == 0 for df in exclusion_trial_dfs):\n",
    "        print(\"‚ö†Ô∏è No exclusion trial dictionary data available\")\n",
    "        return set()\n",
    "\n",
    "    # Prepare code sets for each exclusion dictionary\n",
    "    code_columns = ['code', 'Code', 'CODE', 'system', 'System']\n",
    "    exclusion_code_sets = []\n",
    "    \n",
    "    for trial_df in exclusion_trial_dfs:\n",
    "        code_col = next((col for col in code_columns if col in trial_df.columns), None)\n",
    "        if code_col is None:\n",
    "            print(f\"‚ùå Could not identify code column in exclusion file. Available columns: {list(trial_df.columns)}\")\n",
    "            continue\n",
    "        exclusion_code_sets.append(set(trial_df[code_col].astype(str).str.strip().str.upper()))\n",
    "\n",
    "    if not exclusion_code_sets:\n",
    "        return set()\n",
    "\n",
    "    # Track which patients have exclusion matches\n",
    "    excluded_patients = set()\n",
    "    exclusion_details = defaultdict(list)  # Track what exclusion codes were found for each patient\n",
    "\n",
    "    omop_code_columns = {\n",
    "        'CONDITION_OCCURRENCE': 'condition_source_value',\n",
    "        'DRUG_EXPOSURE': 'drug_source_value',\n",
    "        'MEASUREMENT': 'measurement_source_value',\n",
    "        'PROCEDURE_OCCURRENCE': 'procedure_source_value'\n",
    "    }\n",
    "\n",
    "    print(f\"\\nüîç Checking inclusion-matched patients against exclusion criteria...\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Only check patients who matched inclusion criteria\n",
    "    inclusion_person_ids = set(inclusion_matched_patients.keys())\n",
    "\n",
    "    for table_name, df in omop_tables.items():\n",
    "        code_column = omop_code_columns.get(table_name)\n",
    "        if code_column not in df.columns or 'person_id' not in df.columns:\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nProcessing OMOP table: {table_name}\")\n",
    "        \n",
    "        # Filter to only inclusion-matched patients\n",
    "        df_filtered = df[df['person_id'].isin(inclusion_person_ids)]\n",
    "        \n",
    "        for idx, row in df_filtered.iterrows():\n",
    "            source_code = str(row[code_column]).strip().upper()\n",
    "            person_id = row['person_id']\n",
    "            \n",
    "            # Check against all exclusion code sets\n",
    "            for excl_idx, code_set in enumerate(exclusion_code_sets):\n",
    "                if source_code in code_set:\n",
    "                    excluded_patients.add(person_id)\n",
    "                    exclusion_details[person_id].append({\n",
    "                        'code': source_code,\n",
    "                        'table': table_name,\n",
    "                        'exclusion_list': excl_idx + 1\n",
    "                    })\n",
    "                    print(f\"   ‚ùå EXCLUSION - Patient ID: {person_id}, Code: {source_code}, Exclusion List: {excl_idx+1}\")\n",
    "\n",
    "    return excluded_patients, exclusion_details\n",
    "\n",
    "# Main execution - Apply Exclusion Filtering\n",
    "print(\"\\nüö´ EXCLUSION CRITERIA FILTERING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Find and load all exclusion trial dictionaries\n",
    "exclusion_csv_files = find_latest_exclusion_csvs()\n",
    "\n",
    "if exclusion_csv_files:\n",
    "    print(f\"üìÅ Found {len(exclusion_csv_files)} exclusion trial dictionaries:\")\n",
    "    for f in exclusion_csv_files:\n",
    "        print(f\"   - {f}\")\n",
    "\n",
    "    # Load all exclusion trial dictionaries into DataFrames\n",
    "    exclusion_trial_dfs = []\n",
    "    for csv_file in exclusion_csv_files:\n",
    "        try:\n",
    "            df = pd.read_csv(csv_file)\n",
    "            exclusion_trial_dfs.append(df)\n",
    "            print(f\"   ‚úÖ Loaded {csv_file}: {len(df)} codes\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error loading {csv_file}: {e}\")\n",
    "\n",
    "    if patient_matches and omop_tables and exclusion_trial_dfs:\n",
    "        # Check for exclusions\n",
    "        excluded_patients, exclusion_details = match_exclusion_codes(omop_tables, exclusion_trial_dfs, patient_matches)\n",
    "        \n",
    "        if excluded_patients:\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"‚ùå Found {len(excluded_patients)} patients with exclusion criteria:\")\n",
    "            \n",
    "            # Show sample excluded patients\n",
    "            for idx, person_id in enumerate(list(excluded_patients)[:5]):\n",
    "                details = exclusion_details[person_id]\n",
    "                print(f\"\\n   Patient {idx+1} (person_id: {person_id}):\")\n",
    "                print(f\"     Exclusion matches: {len(details)}\")\n",
    "                for detail in details[:3]:  # Show first 3 matches\n",
    "                    print(f\"       - Code: {detail['code']} (List {detail['exclusion_list']}, Table: {detail['table']})\")\n",
    "            \n",
    "            # Filter out excluded patients from patient_matches\n",
    "            original_count = len(patient_matches)\n",
    "            patient_matches = {pid: data for pid, data in patient_matches.items() if pid not in excluded_patients}\n",
    "            \n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"üìä Cohort Updated After Exclusion Filtering:\")\n",
    "            print(f\"   Patients before exclusion: {original_count}\")\n",
    "            print(f\"   Patients excluded: {len(excluded_patients)}\")\n",
    "            print(f\"   Final cohort size: {len(patient_matches)}\")\n",
    "            print(f\"{'='*80}\")\n",
    "        else:\n",
    "            print(f\"\\n‚úÖ No patients matched exclusion criteria. All {len(patient_matches)} patients remain in cohort.\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Cannot apply exclusion filtering - missing required data\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è No exclusion trial dictionary CSV files found.\")\n",
    "    print(\"   Skipping exclusion filtering - all inclusion-matched patients will be retained.\")\n",
    "    exclusion_csv_files = []\n",
    "\n",
    "print(f\"\\n‚úÖ Exclusion filtering complete. Proceed to next cell for cohort summary.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f09c595",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: Display Final Cohort Summary and Export Results\n",
    "\n",
    "### Generate Comprehensive Statistics for Final Cohort\n",
    "\n",
    "This section displays detailed statistics about the final matched cohort (after both inclusion matching and exclusion filtering) and exports results to CSV.\n",
    "\n",
    "#### Summary Includes:\n",
    "\n",
    "- **Total Patient Count**: Number of patients in OMOP data vs. final matched patients\n",
    "- **Match Percentage**: Percentage of patients meeting ALL inclusion criteria AND having NO exclusion criteria\n",
    "- **Inclusion List Statistics**: Match counts for each individual inclusion criteria list\n",
    "- **Sample Patient Details**: First 5 matched patients with their match counts\n",
    "- **CSV Export**: Results saved to `Output/cohort_matching_results.csv`\n",
    "\n",
    "#### Output Format:\n",
    "\n",
    "The exported CSV contains:\n",
    "- `person_id`: Obfuscated patient identifier\n",
    "- `inclusion_match_counts`: Number of matches per inclusion list (comma-separated)\n",
    "- `total_inclusion_lists`: Total number of inclusion criteria lists\n",
    "\n",
    "**Note**: The final cohort displayed here has already been filtered to exclude patients with any exclusion criteria matches.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ae12c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìä COHORT MATCHING SUMMARY\n",
      "================================================================================\n",
      "\n",
      "üë• Patient Cohort Statistics:\n",
      "   Total patients in RWE data: 35\n",
      "   Patients matching ALL inclusion criteria: 1\n",
      "   Match percentage: 2.9%\n",
      "\n",
      "üéØ Inclusion Criteria Match Distribution:\n",
      "   Total inclusion criteria lists: 2\n",
      "\n",
      "   Inclusion List 1 (Inclusion_18957_codes.csv):\n",
      "     Average matches per patient: 1.0\n",
      "     Min matches: 1\n",
      "     Max matches: 1\n",
      "\n",
      "   Inclusion List 2 (Inclusion_18950_codes.csv):\n",
      "     Average matches per patient: 1.0\n",
      "     Min matches: 1\n",
      "     Max matches: 1\n",
      "\n",
      "üìã Sample Matched Patients (first 5):\n",
      "\n",
      "   Patient 1 (person_id: 443887166406931539):\n",
      "     Matches per inclusion list: [1, 1]\n",
      "     Total inclusion lists matched: 2/2\n",
      "\n",
      "üíæ Results exported to: Output/cohort_matching_results.csv\n",
      "\n",
      "‚úÖ Results stored in 'cohort_matching_results' variable for further analysis\n"
     ]
    }
   ],
   "source": [
    "def display_cohort_summary(patient_matches, omop_tables, inclusion_csv_files):\n",
    "    \"\"\"Display comprehensive cohort matching summary for inclusion criteria\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üìä COHORT MATCHING SUMMARY\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    if not patient_matches:\n",
    "        print(\"‚ùå No patients matched the trial eligibility criteria\")\n",
    "        return\n",
    "    \n",
    "    # Calculate total unique patients in OMOP data\n",
    "    all_person_ids = set()\n",
    "    for table_name, df in omop_tables.items():\n",
    "        if 'person_id' in df.columns:\n",
    "            all_person_ids.update(df['person_id'].unique())\n",
    "    \n",
    "    total_patients = len(all_person_ids)\n",
    "    matched_patients = len(patient_matches)\n",
    "    \n",
    "    print(f\"\\nüë• Patient Cohort Statistics:\")\n",
    "    print(f\"   Total patients in RWE data: {total_patients}\")\n",
    "    print(f\"   Patients matching ALL inclusion criteria: {matched_patients}\")\n",
    "    \n",
    "    if total_patients > 0:\n",
    "        match_percentage = (matched_patients / total_patients) * 100\n",
    "        print(f\"   Match percentage: {match_percentage:.1f}%\")\n",
    "    \n",
    "    # Analyze match distribution across inclusion lists\n",
    "    print(f\"\\nüéØ Inclusion Criteria Match Distribution:\")\n",
    "    if matched_patients > 0:\n",
    "        # Get the number of inclusion lists\n",
    "        num_inclusion_lists = patient_matches[list(patient_matches.keys())[0]]['total_inclusion_lists']\n",
    "        print(f\"   Total inclusion criteria lists: {num_inclusion_lists}\")\n",
    "        \n",
    "        # Calculate average matches per inclusion list\n",
    "        for i in range(num_inclusion_lists):\n",
    "            matches_for_list = [data['inclusion_match_counts'][i] for data in patient_matches.values()]\n",
    "            avg_matches = sum(matches_for_list) / len(matches_for_list) if matches_for_list else 0\n",
    "            min_matches = min(matches_for_list) if matches_for_list else 0\n",
    "            max_matches = max(matches_for_list) if matches_for_list else 0\n",
    "            print(f\"\\n   Inclusion List {i+1} ({inclusion_csv_files[i] if i < len(inclusion_csv_files) else 'Unknown'}):\")\n",
    "            print(f\"     Average matches per patient: {avg_matches:.1f}\")\n",
    "            print(f\"     Min matches: {min_matches}\")\n",
    "            print(f\"     Max matches: {max_matches}\")\n",
    "    \n",
    "    # Show sample matched patients\n",
    "    print(f\"\\nüìã Sample Matched Patients (first 5):\")\n",
    "    for idx, (person_id, data) in enumerate(list(patient_matches.items())[:5]):\n",
    "        print(f\"\\n   Patient {idx+1} (person_id: {person_id}):\")\n",
    "        print(f\"     Matches per inclusion list: {data['inclusion_match_counts']}\")\n",
    "        print(f\"     Total inclusion lists matched: {data['total_inclusion_lists']}/{data['total_inclusion_lists']}\")\n",
    "    \n",
    "    # Save results to CSV\n",
    "    if patient_matches:\n",
    "        results_file = 'Output/cohort_matching_results.csv'\n",
    "        results_list = []\n",
    "        for person_id, data in patient_matches.items():\n",
    "            results_list.append({\n",
    "                'person_id': person_id,\n",
    "                'inclusion_match_counts': ', '.join(map(str, data['inclusion_match_counts'])),\n",
    "                'total_inclusion_lists': data['total_inclusion_lists']\n",
    "            })\n",
    "        df_results = pd.DataFrame(results_list)\n",
    "        df_results.to_csv(results_file, index=False)\n",
    "        print(f\"\\nüíæ Results exported to: {results_file}\")\n",
    "    \n",
    "    # Store results globally\n",
    "    globals()['cohort_matching_results'] = {\n",
    "        'patient_matches': patient_matches,\n",
    "        'inclusion_dictionary_files': inclusion_csv_files,\n",
    "        'omop_tables_used': list(omop_tables.keys())\n",
    "    }\n",
    "    print(f\"\\n‚úÖ Results stored in 'cohort_matching_results' variable for further analysis\")\n",
    "\n",
    "# Display summary and export results\n",
    "if patient_matches and omop_tables:\n",
    "    display_cohort_summary(patient_matches, omop_tables, inclusion_csv_files)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No patient matches to display. Please run Step 3 first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd826e6a-e3fa-49b6-a596-4f0d3d878fde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
