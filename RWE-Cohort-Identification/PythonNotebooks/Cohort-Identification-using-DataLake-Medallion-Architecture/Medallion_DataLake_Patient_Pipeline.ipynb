{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05faa865",
   "metadata": {},
   "source": [
    "<img src=\"./static/imo_health.png\" alt=\"IMO Health Logo\" width=\"300\"/>\n",
    " \n",
    "---\n",
    "# Medallion Data Lake Architecture: Patient Data Pipeline\n",
    "\n",
    "This notebook demonstrates a Medallion architecture for patient data using Amazon S3 and IMO Health APIs.\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                      MEDALLION DATA LAKE ARCHITECTURE                        ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "    ‚îÇ              ‚îÇ         ‚îÇ              ‚îÇ         ‚îÇ              ‚îÇ\n",
    "    ‚îÇ   ü•â BRONZE  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  ü•à SILVER   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   ü•á GOLD    ‚îÇ\n",
    "    ‚îÇ     ZONE     ‚îÇ         ‚îÇ     ZONE     ‚îÇ         ‚îÇ     ZONE     ‚îÇ\n",
    "    ‚îÇ              ‚îÇ         ‚îÇ              ‚îÇ         ‚îÇ              ‚îÇ\n",
    "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "         ‚îÇ                          ‚îÇ                        ‚îÇ\n",
    "         ‚îÇ                          ‚îÇ                        ‚îÇ\n",
    "    Raw Patient              Normalized Data          Cohort-Filtered\n",
    "    Data Ingestion           (IMO Health Precision     Patient Records\n",
    "    (CSV files)              Normalize API)           (Final Analytics)\n",
    "         ‚îÇ                          ‚îÇ                        ‚îÇ\n",
    "         ‚îÇ                          ‚îÇ                        ‚îÇ\n",
    "    ‚Ä¢ Unprocessed          ‚Ä¢ Standardized codes       ‚Ä¢ Business-ready\n",
    "    ‚Ä¢ As-is storage        ‚Ä¢ LOINC, ICD10CM           ‚Ä¢ Criteria-matched\n",
    "    ‚Ä¢ Historical           ‚Ä¢ RXNORM, CPT              ‚Ä¢ Decision support\n",
    "      archive              ‚Ä¢ Quality improved         ‚Ä¢ Reporting\n",
    "```\n",
    "\n",
    "## Pipeline Steps:\n",
    "\n",
    "1. Import required libraries\n",
    "2. Connect to Data Lake Storage (S3)\n",
    "3. Upload files to the Bronze zone\n",
    "4. Read data from the Bronze zone\n",
    "5. Normalize data using IMO Precision Normalize API (Silver zone)\n",
    "6. Apply cohort criteria and write results to the Gold zone\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256c8ab0",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Import libraries such as os, pandas, and boto3 for connecting to Amazon S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d301ec0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if not already installed\n",
    "!pip install pandas boto3 requests ipykernel python-dotenv\n",
    "\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3490d2d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eebbd38f",
   "metadata": {},
   "source": [
    "## 1a. Set (or create .env file) & Load Environment Variables from .env File\n",
    "\n",
    "Use python-dotenv to load environment variables from a .env file for secure and convenient configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308225dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"]= \"Replace\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"]= \"Replace\"\n",
    "os.environ[\"AWS_SESSION_TOKEN\"]= \"Replace\"\n",
    "os.environ[\"IMO_CLIENT_ID\"]= \"Replace\"\n",
    "os.environ[\"IMO_CLIENT_SECRET\"]= \"Replace\"\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9de354",
   "metadata": {},
   "source": [
    "## 2. Connect to Data Lake Storage\n",
    "\n",
    "Establish a connection to Amazon S3 using boto3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969c9857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to S3 using default credential provider chain (recommended)\n",
    "import boto3\n",
    "import os\n",
    "s3 = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),\n",
    "    aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY'),\n",
    "    aws_session_token=os.getenv('AWS_SESSION_TOKEN')  # Optional\n",
    ")\n",
    "bronze_bucket = 'your-bronze-bucket'\n",
    "silver_bucket = 'your-silver-bucket'\n",
    "gold_bucket = 'your-gold-bucket'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11cd964",
   "metadata": {},
   "source": [
    "## 3. Get IMO Health API Auth Token\n",
    "\n",
    "Obtain an authentication token from the IMO OAuth endpoint before making API requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a596430",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "# Get IMO API Auth Token\n",
    "auth_url = \"https://auth.imohealth.com/oauth/token\"\n",
    "client_id = os.getenv('IMO_CLIENT_ID')\n",
    "client_secret = os.getenv('IMO_CLIENT_SECRET')\n",
    "print(client_id)\n",
    "payload = {\n",
    "    'grant_type': 'client_credentials',\n",
    "    'client_id': client_id,\n",
    "    'client_secret': client_secret,\n",
    "    'audience': 'https://api.imohealth.com'\n",
    "}\n",
    "response = requests.post(auth_url, data=payload)\n",
    "if response.status_code == 200:\n",
    "    access_token = response.json().get('access_token')\n",
    "    print(\"Successfully obtained access token.\")\n",
    "else:\n",
    "    print(f\"Failed to obtain access token: {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98caa2d1",
   "metadata": {},
   "source": [
    "## 4. Upload Files in Data Lake\n",
    "\n",
    "List the files and directories available in the Bronze S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbf46ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "from botocore.exceptions import NoCredentialsError, PartialCredentialsError\n",
    "\n",
    "# Upload a CSV file to the Bronze S3 bucket\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "local_file = './DQA_Input.csv'  # Path to your local CSV file\n",
    "bronze_key = 'bronze/patient_data.csv'  # S3 key for the file\n",
    "try:\n",
    "    s3.upload_file(local_file, bronze_bucket, bronze_key)\n",
    "    print(f'Uploaded {local_file} to s3://{bronze_bucket}/{bronze_key}')\n",
    "except (NoCredentialsError, PartialCredentialsError):\n",
    "    print(\"ERROR: AWS credentials not found. Please configure your credentials using AWS CLI or environment variables.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File {local_file} not found. Please check the path.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: {e}\")\n",
    "\n",
    "# List files in the Bronze bucket\n",
    "try:\n",
    "    response = s3.list_objects_v2(Bucket=bronze_bucket, Prefix='bronze/')\n",
    "    for obj in response.get('Contents', []):\n",
    "        print(obj['Key'])\n",
    "except (NoCredentialsError, PartialCredentialsError):\n",
    "    print(\"ERROR: AWS credentials not found. Please configure your credentials using AWS CLI or environment variables.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf501076",
   "metadata": {},
   "source": [
    "## 5. Read Data from Data Lake\n",
    "\n",
    "Read a sample data file from the Bronze S3 bucket into a pandas DataFrame for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d038e476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a sample CSV from Bronze zone\n",
    "bronze_key = 'bronze/patient_data.csv'\n",
    "s3.download_file(bronze_bucket, bronze_key, '/tmp/patient_data.csv')\n",
    "df = pd.read_csv('/tmp/patient_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743bdfcd",
   "metadata": {},
   "source": [
    "## 6. Process and Analyze Data\n",
    "\n",
    "Normalize the data using IMO Precision Normalize API and write the output to the Silver S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5f4c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data using IMO Health Precision Normalize API (batch, following NormalizeRunnerFB.py style)\n",
    "import os\n",
    "import json\n",
    "import uuid\n",
    "\n",
    "# Set up API endpoint and headers (replace with your actual credentials/token)\n",
    "url = \"https://api.imohealth.com/precision/normalize\"\n",
    "headers = {\n",
    "    'Content-Type': 'application/json',\n",
    "    'Accept': 'application/json',\n",
    "    'imo-bypass-cache': 'true',\n",
    "    'imo-origin-client-id': os.getenv('IMO_CLIENT_ID', ''),\n",
    "    'Authorization': f\"Bearer {access_token}\"\n",
    "}\n",
    "\n",
    "def build_requests_array(df):\n",
    "    requests_array = []\n",
    "    for i, row in df.iterrows():\n",
    "        requests_array.append({\n",
    "            \"record_id\": str(row.get('record_id', uuid.uuid4())),\n",
    "            \"domain\": row.get('domain', ''),\n",
    "            \"input_term\": row.get('input_term', ''),\n",
    "            \"context\": {\n",
    "                \"order_name\": row.get('order_name', ''),\n",
    "                \"unit_of_measure\": row.get('component-uom', ''),\n",
    "            }\n",
    "        })\n",
    "    return requests_array\n",
    "\n",
    "batch_size = 10\n",
    "results = []\n",
    "for start in range(0, len(df), batch_size):\n",
    "    batch_df = df.iloc[start:start+batch_size]\n",
    "    requests_array = build_requests_array(batch_df)\n",
    "    payload = json.dumps({\n",
    "        \"client_request_id\": str(uuid.uuid4()),\n",
    "        \"preferences\": {\n",
    "            \"threshold\": 0,\n",
    "            \"debug\": False,\n",
    "            \"use_llm_transformation\": False,\n",
    "            \"use_llm_select_candidate\": False\n",
    "        },\n",
    "        \"requests\": requests_array\n",
    "    })\n",
    "    response = requests.post(url, headers=headers, data=payload)\n",
    "    response_data = response.json()\n",
    "    # Attach results to batch_df\n",
    "    for i, response_item in enumerate(response_data.get('requests', [])):\n",
    "        row = batch_df.iloc[i].to_dict()\n",
    "        resp = response_item.get('response', {})\n",
    "        row['Explanation'] = resp.get('explanation', '')\n",
    "        row['Transformed Term'] = resp.get('transformed_term', '')\n",
    "        # Default values\n",
    "        row['Default Lexical Title'] = ''\n",
    "        row['Default Lexical Code'] = ''\n",
    "        row['Lexical Title'] = ''\n",
    "        row['Lexical Code'] = ''\n",
    "        row['Score'] = 0\n",
    "        row['LOINC'] = ''\n",
    "        row['ICD10CM'] = ''\n",
    "        row['RXNORM'] = ''\n",
    "        row['CPT'] = ''\n",
    "        if resp.get('items') and len(resp['items']) > 0:\n",
    "            item = resp['items'][0]\n",
    "            row['Default Lexical Title'] = item.get('default_lexical_title', '')\n",
    "            row['Default Lexical Code'] = item.get('default_lexical_code', '')\n",
    "            row['Lexical Title'] = item.get('lexical_title', '')\n",
    "            row['Lexical Code'] = item.get('lexical_code', '')\n",
    "            row['Score'] = item.get('score', 0)\n",
    "            metadata = item.get('metadata', {})\n",
    "            # Extract codes from metadata['mappings'] if present\n",
    "            try:\n",
    "                mappings = metadata.get('mappings', {})\n",
    "                # LOINC for lab domain\n",
    "                if row.get('domain', '').lower() == 'lab':\n",
    "                    loinc = mappings.get('loinc', {})\n",
    "                    loinc_codes = loinc.get('codes', [])\n",
    "                    if loinc_codes and isinstance(loinc_codes, list):\n",
    "                        row['LOINC'] = loinc_codes[0].get('code', '')\n",
    "                # ICD10CM for problem domain\n",
    "                if row.get('domain', '').lower() == 'problem':\n",
    "                    icd10cm = mappings.get('icd10cm', {})\n",
    "                    icd10cm_codes = icd10cm.get('codes', [])\n",
    "                    if icd10cm_codes and isinstance(icd10cm_codes, list):\n",
    "                        row['ICD10CM'] = icd10cm_codes[0].get('code', '')\n",
    "                # RXNORM for medication domain\n",
    "                if row.get('domain', '').lower() == 'medication':\n",
    "                    rxnorm = mappings.get('rxnorm', {})\n",
    "                    rxnorm_codes = rxnorm.get('codes', [])\n",
    "                    if rxnorm_codes and isinstance(rxnorm_codes, list):\n",
    "                        row['RXNORM'] = rxnorm_codes[0].get('code', '')\n",
    "                # CPT for procedure domain\n",
    "                if row.get('domain', '').lower() == 'procedure':\n",
    "                    cpt = mappings.get('cpt', {})\n",
    "                    cpt_codes = cpt.get('codes', [])\n",
    "                    if cpt_codes and isinstance(cpt_codes, list):\n",
    "                        row['CPT'] = cpt_codes[0].get('code', '')\n",
    "            except Exception as e:\n",
    "                pass\n",
    "        results.append(row)\n",
    "\n",
    "norm_df = pd.DataFrame(results)\n",
    "norm_df.to_csv('/tmp/normalized.csv', index=False)\n",
    "s3.upload_file('/tmp/normalized.csv', silver_bucket, 'silver/normalized_patient_data.csv')\n",
    "print('Normalized data written to Silver zone. LOINC, ICD10CM, RXNORM, CPT columns extracted from metadata.mappings.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bdcaf6",
   "metadata": {},
   "source": [
    "## 6. Preview Normalized Data in Silver Zone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653e0b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the normalized dataset from the Silver zone before cohorting\n",
    "s3.download_file(silver_bucket, 'silver/normalized_patient_data.csv', '/tmp/normalized.csv')\n",
    "norm_df = pd.read_csv('/tmp/normalized.csv')\n",
    "display(norm_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd46929b",
   "metadata": {},
   "source": [
    "## 7. Select and Download Cohort Criteria definition\n",
    "1.  Select Cohort Criteria defined in  IMO Health Precision Set Platform:\n",
    "\n",
    "        - https://studio.imohealth.com/#/valuesets/editor/individual/20294/version/66922\n",
    "        - Scope: Finding all tests related to  anti mitochondria antibody\n",
    "\n",
    "2. Download the valueset defined\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cfe2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "\n",
    "def search_valuesets(search_term, token):\n",
    "    \"\"\"Search for valuesets using the IMO Health API\"\"\"\n",
    "    url = \"https://api.imohealth.com/fhir/r6/ValueSet/search\"\n",
    "    \n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'Authorization': f'Bearer {token}'\n",
    "    }\n",
    "    params = {\"searchText\": search_term}\n",
    "\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        raise Exception(f\"Failed to search valuesets: {response.status_code} - {response.text}\")\n",
    "\n",
    "def get_valueset_codes_paged(valueset_id, token):\n",
    "    \"\"\"Get all codes from a valueset with pagination\"\"\"\n",
    "    all_codes = []\n",
    "    page = 1\n",
    "    page_size = 50\n",
    "    max_pages = 100\n",
    "    \n",
    "    while page <= max_pages:\n",
    "        url = f\"https://api.imohealth.com/fhir/r6/ValueSet/{valueset_id}\"\n",
    "        \n",
    "        headers = {\n",
    "            'Authorization': f'Bearer {token}'\n",
    "        }\n",
    "        \n",
    "        params = {\n",
    "            'page': page,\n",
    "            'pageSize': page_size\n",
    "        }\n",
    "        \n",
    "        response = requests.get(url, headers=headers, params=params)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"Failed to get valueset codes: {response.status_code} - {response.text}\")\n",
    "        \n",
    "        data = response.json()\n",
    "        expansion = data.get('expansion', {})\n",
    "        total = expansion.get('total', 0)   \n",
    "        codes = expansion.get('contains', [])\n",
    "\n",
    "        if not codes:\n",
    "            break\n",
    "            \n",
    "        all_codes.extend(codes)\n",
    "        print(f\"Retrieved page {page}, codes on this page: {len(codes)}, total codes so far: {len(all_codes)}\")\n",
    "        \n",
    "        if len(all_codes) >= total or len(codes) < page_size:\n",
    "            print(f\"Pagination complete. Total codes retrieved: {len(all_codes)}\")\n",
    "            break\n",
    "            \n",
    "        page += 1\n",
    "    \n",
    "    if page > max_pages:\n",
    "        print(f\"‚ö†Ô∏è Reached maximum page limit ({max_pages}). Retrieved {len(all_codes)} codes.\")\n",
    "    \n",
    "    return all_codes\n",
    "\n",
    "def save_codes_to_csv(codes, filename):\n",
    "    \"\"\"Save codes to CSV file\"\"\"\n",
    "    filepath = os.path.join('.', filename)\n",
    "    \n",
    "    with open(filepath, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        if codes:\n",
    "            fieldnames = codes[0].keys()\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(codes)\n",
    "    \n",
    "    return filepath\n",
    "\n",
    "# Download precision set for anti mitochondria antibody tests\n",
    "valueset_id = \"20294\"  # ID from the precision set URL\n",
    "print(f\"üì• Downloading precision set (ID: {valueset_id}) for anti mitochondria antibody tests...\")\n",
    "\n",
    "try:\n",
    "    codes = get_valueset_codes_paged(valueset_id, access_token)\n",
    "    \n",
    "    if codes:\n",
    "        filename = f\"precision_set_{valueset_id}_codes.csv\"\n",
    "        filepath = save_codes_to_csv(codes, filename)\n",
    "        \n",
    "        print(f\"‚úÖ Successfully downloaded {len(codes)} codes!\")\n",
    "        print(f\"üìÅ File saved as: {filename}\")\n",
    "        \n",
    "        # Load into DataFrame for cohort matching\n",
    "        valueset_df = pd.DataFrame(codes)\n",
    "        print(f\"\\nüìä Valueset preview:\")\n",
    "        display(valueset_df.head())\n",
    "        \n",
    "        # Extract codes into a set for matching\n",
    "        valueset_codes = set(valueset_df['code'].values)\n",
    "        print(f\"\\n‚úÖ Loaded {len(valueset_codes)} unique codes for cohort matching\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No codes found for this valueset.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error downloading precision set: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ee1a92",
   "metadata": {},
   "source": [
    "## Apply Cohorting Rules based on the downloaded precision set and copy the finalized output to the Gold Zone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897ad10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply cohorting rules based on the downloaded precision set\n",
    "# Filter normalized DataFrame for records matching the valueset codes\n",
    "cohort_df = norm_df[norm_df['LOINC'].isin(valueset_codes)]\n",
    "print(f\"Total records in cohort matching precision set: {len(cohort_df)}\")\n",
    "cohort_df.to_csv('/tmp/cohort_data.csv', index=False)\n",
    "\n",
    "## Move to Gold zone\n",
    "s3.upload_file('/tmp/cohort_data.csv', gold_bucket, 'gold/cohort_patient_data.csv')\n",
    "print('Cohort data written to Gold zone.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4308ddf",
   "metadata": {},
   "source": [
    "## 8. Preview Final Cohort Data in Gold Zone\n",
    "\n",
    "Display the final cohorted patient data that was written to the Gold zone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396f831a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the final cohort data from the Gold zone\n",
    "s3.download_file(gold_bucket, 'gold/cohort_patient_data.csv', '/tmp/cohort_data.csv')\n",
    "gold_df = pd.read_csv('/tmp/cohort_data.csv')\n",
    "print(f\"üìä Total records in Gold zone: {len(gold_df)}\")\n",
    "print(f\"\\nüéØ Final cohort data preview:\")\n",
    "display(gold_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b010433",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated a Medallion Data Lake pipeline for patient data using S3 and IMO APIs:\n",
    "\n",
    "- Raw data ingested to Bronze zone\n",
    "- Normalized to Silver zone using IMO Precision Normalize API\n",
    "- Cohort criteria applied and results moved to Gold zone"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
