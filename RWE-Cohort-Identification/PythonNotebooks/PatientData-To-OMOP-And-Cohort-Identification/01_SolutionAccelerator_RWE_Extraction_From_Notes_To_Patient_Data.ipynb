{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "833934ef",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/imohealth/solution-engineering/refs/heads/updates/RWE-Cohort-Identification/PythonNotebooks/PatientData-To-OMOP-And-Cohort-Identification/static/imo_health.png?token=GHSAT0AAAAAADSTDGZJSSJF42RDYZPNU2YY2LYZN7Q\" alt=\"IMO Health Logo\" width=\"300\"/>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3189cadb",
   "metadata": {},
   "source": [
    "# RWE Patient Data Extraction from Clinical Notes\n",
    "\n",
    "This notebook is the **first step** in the RWE Cohort Identification pipeline. It processes unstructured clinical notes through IMO's NLP API to extract structured patient data.\n",
    "\n",
    "## Workflow Overview\n",
    "\n",
    "This notebook performs NLP-based entity extraction from IHM (Intelligent Health Management) clinical notes:\n",
    "\n",
    "1. **CSV Data Loading**: Load unstructured clinical notes from CSV files in `upload_ihm` folder\n",
    "2. **API Authentication**: Authenticate with IMO's API services using Auth0 credentials\n",
    "3. **Input Validation**: Verify CSV structure and required fields (hospitalGuid, encounterGuid, patientGuid, notesDEID)\n",
    "4. **NLP Entity Extraction**: Process notes through IMO's Clinical Comprehensive NLP pipeline to extract:\n",
    "   - **Problems/Diagnoses**: Medical conditions with ICD10CM codes\n",
    "   - **Medications**: Drug exposures with RXNORM codes\n",
    "   - **Lab Tests**: Measurements with LOINC codes\n",
    "   - **Procedures**: Clinical procedures with CPT codes\n",
    "5. **Structured Output**: Generate `patient_output.xlsx` with extracted entities and standard medical codes\n",
    "\n",
    "## Output\n",
    "\n",
    "The extracted patient data (`patient_output.xlsx`) serves as input for the next step in the pipeline:\n",
    "- **Next Step**: OMOP conversion using `Patient_Data_to_OMOP_Converter.ipynb`\n",
    "- **Final Step**: RWE gneration by matching with eligibility criteria defined in IMO Precision Set Editor\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- IMO API credentials configured in `config.json`\n",
    "- CSV files with clinical notes in `upload_ihm/` directory\n",
    "- Required Python packages: `pandas`, `openpyxl`, `xlsxwriter`, `requests`\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1: Environment Setup and Configuration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80566eeb",
   "metadata": {},
   "source": [
    "### Verify Required Packages\n",
    "\n",
    "This cell verifies that all necessary Python packages are installed:\n",
    "\n",
    "- **`pandas`**: Data manipulation and analysis library\n",
    "- **`openpyxl`**: Reading/writing Excel files\n",
    "- **`xlsxwriter`**: Creating formatted Excel files\n",
    "- **`requests`**: HTTP library for API calls to IMO services\n",
    "\n",
    "> **Note**: These packages should already be installed in your virtual environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548faa13-c63b-448f-b2bb-b16ec14fc65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Install required packages (uncomment if running in a new environment)\n",
    "# Install required packages (uncomment if running in a new environment)\n",
    "%pip install boto3 requests  pandas  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493494cd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: Load API Configuration\n",
    "\n",
    "### Load IMO API Credentials\n",
    "\n",
    "This cell loads the API credentials from `config.json` and obtains an authentication token:\n",
    "\n",
    "- **Auth0 Authentication**: Authenticates with IMO's Auth0 service\n",
    "- **Bearer Token**: Retrieves access token for API calls\n",
    "- **Token Storage**: Stores token globally for use in subsequent steps\n",
    "\n",
    "The configuration file should contain:\n",
    "- `client_id`: Your IMO API client ID\n",
    "- `client_secret`: Your IMO API client secret\n",
    "- `audience`: API audience identifier\n",
    "- `domain`: Auth0 domain\n",
    "\n",
    "This token will be used for both NLP entity extraction and FHIR valueset searches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c8a22d-33c6-4ec0-ab77-f72324616707",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config():\n",
    "    \"\"\"Load configuration from config.json file\"\"\"\n",
    "    # When running from this notebook (in using-OMOP), config.json is one folder up\n",
    "    if '__file__' in globals():\n",
    "        config_path = os.path.join(os.path.dirname(__file__), 'config.json')\n",
    "    else:\n",
    "        config_path = os.path.join('..', 'config.json')\n",
    "    \n",
    "    with open(config_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def get_auth0_token(config):\n",
    "    \"\"\"Get access token from Auth0\"\"\"\n",
    "    auth0_config = config['auth0']\n",
    "    \n",
    "    payload = {\n",
    "        'client_id': auth0_config['client_id'],\n",
    "        'client_secret': auth0_config['client_secret'],\n",
    "        'audience': auth0_config['audience'],\n",
    "        'grant_type': 'client_credentials'\n",
    "    }\n",
    "    \n",
    "    headers = {'content-type': 'application/json'}\n",
    "    token_url = f\"https://{auth0_config['domain']}/oauth/token\"\n",
    "    \n",
    "    response = requests.post(token_url, json=payload, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        token_data = response.json()\n",
    "        return token_data['access_token']\n",
    "    else:\n",
    "        raise Exception(f\"Failed to get token: {response.status_code} - {response.text}\")\n",
    "\n",
    "# Load configuration and get token\n",
    "config = load_config()\n",
    "Token = get_auth0_token(config)\n",
    "\n",
    "print(\"âœ… Successfully authenticated with IMO API\")\n",
    "print(f\"   Token obtained: {Token[:20]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ab55e0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Verify Input Data\n",
    "\n",
    "### Check CSV Files in upload_ihm Folder\n",
    "\n",
    "This cell verifies that the required CSV files are present in the `upload_ihm` directory:\n",
    "\n",
    "1. **Directory Check**: Ensures `upload_ihm` folder exists\n",
    "2. **File Discovery**: Lists all CSV files in the directory\n",
    "3. **Data Preview**: Shows sample records from the CSV file\n",
    "4. **Column Validation**: Verifies required columns (hospitalGuid, encounterGuid, patientGuid, notesDEID)\n",
    "\n",
    "**Expected Input**: CSV file with unstructured clinical notes containing:\n",
    "- `hospitalGuid`: Unique hospital identifier\n",
    "- `encounterGuid`: Unique encounter identifier  \n",
    "- `patientGuid`: Unique patient identifier\n",
    "- `notesDEID`: De-identified clinical notes text\n",
    "\n",
    "> **Important**: Ensure your CSV files are placed in the `upload_ihm/` directory before running this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdd0ec6-c97a-4f84-96ef-9d8b18d244a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify CSV files in upload_ihm folder\n",
    "upload_folder = '../upload_notes'\n",
    "\n",
    "if not os.path.exists(upload_folder):\n",
    "    print(f\"âŒ Error: '{upload_folder}' directory not found!\")\n",
    "    print(f\"   Please create the directory and place your CSV files there.\")\n",
    "else:\n",
    "    csv_files = [f for f in os.listdir(upload_folder) if f.endswith('.csv')]\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(f\"âŒ No CSV files found in '{upload_folder}' directory\")\n",
    "    else:\n",
    "        print(f\"âœ… Found {len(csv_files)} CSV file(s) in '{upload_folder}':\")\n",
    "        for csv_file in csv_files:\n",
    "            file_path = os.path.join(upload_folder, csv_file)\n",
    "            print(f\"\\nðŸ“ File: {csv_file}\")\n",
    "            \n",
    "            # Load and preview the CSV\n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "                print(f\"   Records: {len(df)}\")\n",
    "                print(f\"   Columns: {list(df.columns)}\")\n",
    "                \n",
    "                # Check for required columns\n",
    "                required_cols = ['hospitalGuid', 'encounterGuid', 'patientGuid', 'notesDEID']\n",
    "                missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "                \n",
    "                if missing_cols:\n",
    "                    print(f\"   âš ï¸ Warning: Missing columns: {missing_cols}\")\n",
    "                else:\n",
    "                    print(f\"   âœ… All required columns present\")\n",
    "                    \n",
    "                # Show sample data\n",
    "                print(f\"\\n   Sample records:\")\n",
    "                print(df.head(3).to_string(index=False, max_colwidth=50))\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   âŒ Error reading file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e5d132",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: NLP Entity Extraction - Core Processing\n",
    "\n",
    "### Extract Structured Patient Data from Unstructured Clinical Notes\n",
    "\n",
    "This is the **core processing step** that transforms unstructured IHM clinical notes into structured patient data using IMO's NLP API.\n",
    "\n",
    "#### Processing Workflow:\n",
    "\n",
    "1. **Input Validation**: Reads and validates CSV file from `upload_ihm` folder\n",
    "2. **Note-by-Note Processing**: Iterates through each patient note (notesDEID column)\n",
    "3. **NLP API Calls**: Sends clinical text to IMO's Clinical Comprehensive pipeline\n",
    "4. **Entity Recognition**: Uses advanced NLP to identify:\n",
    "   - Medical problems and diagnoses\n",
    "   - Medications and drug exposures\n",
    "   - Laboratory tests and results\n",
    "   - Clinical procedures\n",
    "5. **Code Mapping**: Automatically maps extracted entities to standard medical vocabularies:\n",
    "   - **ICD10CM**: International Classification of Diseases codes for conditions\n",
    "   - **RXNORM**: Standardized medication codes\n",
    "   - **LOINC**: Logical Observation Identifiers for lab tests\n",
    "   - **CPT**: Current Procedural Terminology codes\n",
    "6. **GUID Preservation**: Maintains linkage via hospitalGuid, encounterGuid, and patientGuid\n",
    "7. **Structured Output**: Creates `patient_output.xlsx` in the `Output` folder\n",
    "\n",
    "#### Key Features:\n",
    "\n",
    "- **Rate Limiting**: Built-in 0.5s delay between API calls to prevent throttling\n",
    "- **Error Handling**: Automatic retry logic for rate limit errors (429 responses)\n",
    "- **Progress Tracking**: Real-time progress updates every 10 records\n",
    "- **Data Quality**: Preserves original GUIDs for downstream OMOP conversion\n",
    "\n",
    "#### Output File Structure (`patient_output.xlsx`):\n",
    "- **Column A-C**: hospitalGuid, encounterGuid, patientGuid (preserved from input)\n",
    "- **Subsequent Columns**: Extracted medical entities with:\n",
    "  - Entity text as it appears in the note\n",
    "  - Standard medical codes (ICD10CM, RXNORM, LOINC, CPT)\n",
    "  - Entity descriptions and clinical context\n",
    "  - Semantic tags (problem, medication, lab, procedure)\n",
    "\n",
    "> **Important**: This extracted patient data file is required for the next step (OMOP conversion). The `Patient_Data_to_OMOP_Converter.ipynb` notebook will read this file and transform it into OMOP CDM v6.0 format.\n",
    "\n",
    "> **Note**: Processing time depends on the number of records. With 0.5s delay per record, expect approximately 25-30 seconds for 50 records.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022275a4-bc06-4ff0-a626-24f49c6625ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run NLP Entity Extraction Process\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Ensure we can import shared.py from the parent folder\n",
    "try:\n",
    "    from shared import create_workbook\n",
    "except ModuleNotFoundError:\n",
    "    sys.path.insert(0, os.path.abspath('..'))\n",
    "    from shared import create_workbook\n",
    "\n",
    "print(\"ðŸš€ Starting NLP Entity Extraction Process...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Configuration\n",
    "csv_file_path = '../upload_ihm/unstructured_IHM.csv'\n",
    "output_workbook_name = 'patient_output.xlsx'  # create_workbook adds Output/ prefix\n",
    "url = 'https://api.imohealth.com/entityextraction/pipelines/imo-clinical-comprehensive?version=3.0'\n",
    "domain_filter = [\"problem\", \"procedure\", \"medication\", \"lab\"]\n",
    "score_threshold = 0.0\n",
    "request_delay = 0.5  # seconds between API calls to avoid rate limiting\n",
    "\n",
    "def invoke_api(file_contents, auth_token):\n",
    "    \"\"\"Call IMO NLP API for entity extraction\"\"\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {auth_token}\"\n",
    "    }\n",
    "    request_body = {\n",
    "        \"text\": file_contents,\n",
    "        \"preferences\": {\n",
    "            \"thresholds\": {\"global\": score_threshold},\n",
    "            \"domain_filter\": domain_filter\n",
    "        },\n",
    "    }\n",
    "    response = requests.post(url, data=json.dumps(request_body), headers=headers)\n",
    "    response.raise_for_status()\n",
    "    return response\n",
    "\n",
    "try:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Check if CSV file exists\n",
    "    if not os.path.exists(csv_file_path):\n",
    "        print(f\"âŒ Error: CSV file not found at {csv_file_path}\")\n",
    "        print(f\"   Please ensure the file exists in the upload_ihm folder\")\n",
    "    else:\n",
    "        # Read CSV file\n",
    "        print(f\"ðŸ“ Reading CSV file: {csv_file_path}\")\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "        print(f\"   Total records to process: {len(df)}\")\n",
    "        \n",
    "        # Verify required columns\n",
    "        required_cols = ['hospitalGuid', 'encounterGuid', 'patientGuid', 'notesDEID']\n",
    "        missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "        \n",
    "        if missing_cols:\n",
    "            print(f\"âŒ Error: Missing required columns: {missing_cols}\")\n",
    "        else:\n",
    "            # Process each note through the API\n",
    "            api_responses = {}  # Dictionary format expected by create_workbook\n",
    "            \n",
    "            print(f\"\\nðŸ”„ Processing notes through IMO NLP API...\")\n",
    "            print(f\"   (Adding {request_delay}s delay between requests to avoid rate limiting)\")\n",
    "            \n",
    "            for idx, row in df.iterrows():\n",
    "                note_text = row['notesDEID']\n",
    "                hospital_guid = row['hospitalGuid']\n",
    "                encounter_guid = row['encounterGuid']\n",
    "                patient_guid = row['patientGuid']\n",
    "                \n",
    "                # Create unique identifier for this record\n",
    "                record_id = f\"record_{idx+1}\"\n",
    "                \n",
    "                try:\n",
    "                    # Call API\n",
    "                    api_start = time.time()\n",
    "                    response = invoke_api(note_text, Token)\n",
    "                    api_elapsed = time.time() - api_start\n",
    "                    \n",
    "                    response_data = response.json()\n",
    "                    \n",
    "                    # Store response in format expected by create_workbook\n",
    "                    api_responses[record_id] = {\n",
    "                        'payload': response_data,\n",
    "                        'hospitalGuid': hospital_guid,\n",
    "                        'encounterGuid': encounter_guid,\n",
    "                        'patientGuid': patient_guid,\n",
    "                        'run_time': f\"{api_elapsed:.2f}s\"\n",
    "                    }\n",
    "                    \n",
    "                    if (idx + 1) % 10 == 0:\n",
    "                        print(f\"   Processed {idx + 1}/{len(df)} records...\")\n",
    "                    \n",
    "                    # Add delay to avoid rate limiting\n",
    "                    time.sleep(request_delay)\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    error_msg = str(e)\n",
    "                    if \"429\" in error_msg:\n",
    "                        print(f\"   âš ï¸ Rate limit reached at record {idx + 1}. Waiting 2 seconds...\")\n",
    "                        time.sleep(2)\n",
    "                        # Retry once\n",
    "                        try:\n",
    "                            response = invoke_api(note_text, Token)\n",
    "                            response_data = response.json()\n",
    "                            api_responses[record_id] = {\n",
    "                                'payload': response_data,\n",
    "                                'hospitalGuid': hospital_guid,\n",
    "                                'encounterGuid': encounter_guid,\n",
    "                                'patientGuid': patient_guid,\n",
    "                                'run_time': 'retry'\n",
    "                            }\n",
    "                            print(f\"   âœ… Successfully retried record {idx + 1}\")\n",
    "                        except Exception as retry_error:\n",
    "                            print(f\"   âŒ Retry failed for record {idx + 1}: {str(retry_error)}\")\n",
    "                    else:\n",
    "                        print(f\"   âš ï¸ Error processing record {idx + 1}: {error_msg}\")\n",
    "                    continue\n",
    "            \n",
    "            # Create Excel workbook with results\n",
    "            print(f\"\\nðŸ’¾ Creating output workbook...\")\n",
    "            create_workbook(output_workbook_name, api_responses)\n",
    "            \n",
    "            elapsed_time = time.time() - start_time\n",
    "            \n",
    "            print(f\"\\nâœ… NLP processing completed successfully!\")\n",
    "            print(f\"   Processed records: {len(api_responses)}/{len(df)}\")\n",
    "            print(f\"   Elapsed time: {elapsed_time:.1f} seconds\")\n",
    "            print(f\"   Output file: Output/{output_workbook_name}\")\n",
    "            \n",
    "            # Verify and preview output\n",
    "            output_path = f'Output/{output_workbook_name}'\n",
    "            if os.path.exists(output_path):\n",
    "                df_output = pd.read_excel(output_path, sheet_name='Results')\n",
    "                print(f\"\\nðŸ“Š Output Summary:\")\n",
    "                print(f\"   Total records: {len(df_output)}\")\n",
    "                print(f\"   Columns: {len(df_output.columns)}\")\n",
    "                \n",
    "                if 'Semantic Tag' in df_output.columns:\n",
    "                    print(f\"\\n   Extracted entities by type:\")\n",
    "                    for tag, count in df_output['Semantic Tag'].value_counts().items():\n",
    "                        print(f\"     {tag}: {count}\")\n",
    "            else:\n",
    "                print(f\"\\nâš ï¸ Warning: Output file not created at {output_path}\")\n",
    "                \n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ Error during NLP processing: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5b77ee",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âœ… NLP Extraction Complete - Next Steps\n",
    "\n",
    "### Successfully Extracted Patient Data\n",
    "\n",
    "Congratulations! You have successfully extracted structured patient data from unstructured clinical notes using IMO's NLP API.\n",
    "\n",
    "#### What You've Accomplished:\n",
    "\n",
    "- âœ… Processed IHM clinical notes through IMO's Clinical Comprehensive NLP pipeline\n",
    "- âœ… Extracted medical entities (problems, medications, labs, procedures)\n",
    "- âœ… Mapped entities to standard medical vocabularies (ICD10CM, RXNORM, LOINC, CPT)\n",
    "- âœ… Generated structured patient data in `Output/patient_output.xlsx`\n",
    "- âœ… Preserved patient/encounter/hospital GUIDs for data linkage\n",
    "\n",
    "#### Output File Location:\n",
    "ðŸ“ **`Output/patient_output.xlsx`**\n",
    "\n",
    "This file contains all extracted medical entities with standard codes and is ready for OMOP conversion.\n",
    "\n",
    "---\n",
    "\n",
    "### Next Steps in the RWE Pipeline:\n",
    "\n",
    "#### Step 2: OMOP Conversion\n",
    "**Notebook**: `Patient_Data_to_OMOP_Converter.ipynb`\n",
    "\n",
    "Transform the extracted patient data into OMOP Common Data Model (CDM) v6.0 format:\n",
    "- Convert to standardized OMOP clinical tables (PERSON, VISIT_OCCURRENCE, CONDITION_OCCURRENCE, etc.)\n",
    "- Map medical codes to OMOP standard concepts\n",
    "- Apply data obfuscation for privacy protection\n",
    "- Generate CSV files ready for clinical data warehouses\n",
    "\n",
    "**Input**: `Output/patient_output.xlsx` (generated by this notebook)  \n",
    "**Output**: `Output/OMOP_CSV/*.csv` (OMOP CDM tables)\n",
    "\n",
    "#### Step 3: Cohort Identification \n",
    "- Generate cohort matching reports\n",
    "\n",
    "**Input**: `Output/OMOP_CSV/*.csv` (from Step 2)  \n",
    "**Output**: `Output/cohort_matching_results.csv`\n",
    "\n",
    "---\n",
    "\n",
    "### File Summary:\n",
    "\n",
    "| File | Purpose | Next Notebook |\n",
    "|------|---------|---------------|\n",
    "| `patient_output.xlsx` | Structured patient data with medical codes | `Patient_Data_to_OMOP_Converter.ipynb` |\n",
    "| Original CSV | Source clinical notes | *(Processing complete)* |\n",
    "\n",
    "> **Tip**: Keep the `Output/patient_output.xlsx` file - it's required for OMOP conversion in the next step!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614a375c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
